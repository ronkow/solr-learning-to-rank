{% extends 'base.html' %}

{% block title %}Models{% endblock title %}

{% block mycss %}
{% endblock %}

{% block content %}

<h1>Models</h1>

<h5>In this section, we describe our implementation of machine learning ranking methods known as <a href="https://en.wikipedia.org/wiki/Learning_to_rank">Learning To Rank</a>.
We describe how we measure textual and syntactic similarity between query and document, describe the features for each ranking model, and explain how we create the datasets for the learning algorithm.</h5>

<h4><strong>1. Ranking models</strong></h4>

<p>We use the <a href="https://lucene.apache.org/solr/">Apache Solr</a> search platform combined with a <a href="https://sourceforge.net/p/lemur/wiki/RankLib/">LambdaMART</a>
ranking model to intelligently rank search results. The goal is to develop a model that learns general grammar syntax and produce better rankings
than the default Solr rankings which are computed based on only textual similarity.
There are two different ranking models in our system, depending on whether the answer is a query input:</p>

<div class="container-page-intro">

<p><strong>Model 1:</strong> This model uses features extracted from the entire query.</p>

<p><strong>Model 2:</strong> This model uses features extracted from a sub-string of the query.
The answer is indicated in the query and the sub-string consists of the answer, four (or less) words before the answer, and four (or less) words after the answer.</p>

</div>

<p>
The choice of a four-word context is based on the observation that truncating any sentence such that there are four words before and after the answer often result in a fairly meaningful shorter sentence.
Here are two examples:
</p>
<table>
    <tr>
        <td><span class="table-first-column">Complete sentence</span></td>
        <td><span class="sentence">He sat on the chair <span class="answer">by</span> the window to look at the clouds.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Truncated sentence</span></td>
        <td><span class="sentence">sat on the chair <span class="answer">by</span> the window to look</span></td>
    </tr>
</table>
<table>
    <tr>
        <td><span class="table-first-column">Complete sentence</span></td>
        <td><span class="sentence">The company is expanding fast and <span class="answer">has opened</span> an office in India.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Truncated sentence</span></td>
        <td><span class="sentence">is expanding fast and <span class="answer">has opened</span> an office in India</span></td>
    </tr>
</table>

<hr>
<div class="" id="discuss">

<h4><strong>2. Similarity measures</strong></h4>

<p>Some words can be used as both prepositions and conjunctions. Phrasal verbs and verb tenses both contain verbs.
Phrasal verbs are formed by joining a verb and a preposition. Such relationships between different types of grammatical forms make ranking of results by textual similarity inadequate.
For example, if a user enters a sentence in which the word "after" is used as a preposition, textual similarity would not distinguish between sentences in which "after" is a preposition or a conjunction, or part of a phrasal verb such as "take after".
</p>

<p>We want the system to be able to favourably rank sentences which have very few or no common words with the query, but which have a similar syntactic structure (that is, how the words are arranged together).
We also want the system to look for sentences with words or phrases that have the same parts of speech. For example, these two sentences have only one common word "to" but their syntactic structures are identical:
</p>

<ul class="list-indent">
    <li><span class="example">I am taking my two daughters to school this afternoon.</li>
    <li><span class="example">She is driving her three kids to class later today.</li>
</ul>

<p>
In other words, in addition to textual similarity, the ranking model also needs to be able to detect syntactic similarity.
To achieve this, we use constituency parse trees and parts of speech tags.
But first, we talk about BM25, as BM25 is a crucial part of how we compute our feature values. </p>

<hr>

<h5><strong>Textual similarity: BM25</strong></h5>

<p>
 <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> is the default ranking function in Lucene, the search library from which Solr is built on. We use BM25 to score any textual similarity between query and document.
As we will see later, we also use BM25 to score syntactic similarities between query and document.</p>

<p>The BM25 function is:</p>

<p>
\[\textrm{score}(q,d) = \sum_{t=1}^{n} \textrm{idf}_{t} \cdot \frac{ (k_{1}+1) \cdot \textrm{tf}_{t,d} } { k_{1} \left( 1-b+b \cdot \frac{L_{d}}{L_{\textrm{mean}}} \right) + \textrm{tf}_{t,d} } \]
</p>
<p>where</p>

<div class="formula">
<p>\(q\) is a query containing terms \(t_{1},...,t_{n}\)</p>
<p>\(t\) is a term in \(q\) (we write \(t\) instead of \(t_{i}\) for simplicity)</p>
<p>\(d\) is a document</p>
<p>\(\textrm{tf}_{t,d}\) is the term frequency of term \(t\) in document \(d\)</p>
<p>\(\textrm{idf}_{t}\) is the inverse-document frequency of term \(t\)</p>
<p>\(L_{d}\) is the length (the number of words) of document \(d\)</p>
<p>\(L_{\textrm{mean}}\) is the mean length of all documents in the collection</p>
<p>\(k_{1}\) and \(b\) are tuning parameters (in <a href="https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html">Lucene</a>, default values are \(k_{1}=1.2\), \(b=0.75\))</p>
</div>

<p>Various <a href="http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf">versions of BM25</a> have been proposed; the variations are usually in the \(\textrm{idf}_{t}\) term.
In <a href="https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables">Lucene</a>, \(\textrm{idf}_{t}\) is computed as:</p>

<p>\[\textrm{idf}_{t}=\ln \left( \dfrac{N - \textrm{df}_{t} + 0.5}{\textrm{df}_{t} + 0.5} + 1 \right) \]</p>

<p>where</p>

<div class="formula">
<p>\(N\) is the number of documents in the collection</p>
<p>\(\textrm{df}_{t}\) is the number of documents containing the term \(t\)</p>
</div>

<p>The addition of 1 to \( \dfrac{N - \textrm{df}_{t} + 0.5}{\textrm{df}_{t} + 0.5} \) ensures that \(\textrm{idf}_{t}\) is positive for all \( \textrm{df}_{t} \le N \).</p>

<p>
BM25 is superior to TF-IDF because it allows us to set the values of two parameters which control the behaviour of the query-document score.
The parameter \(k_{1}\) controls the influence of the term frequency.
If \(k_{1}=0\), \(\textrm{tf}_{t,d}\) cancels out and has no effect on the score.
As \(k_{1}\) increases, the influence of \(\textrm{tf}_{t,d}\) increases.
The parameter \(b\) penalises longer documents, helping to eliminate the advantage that longer documents have over shorter documents with regards to term frequency (as longer documents have more terms).
 However, for our short documents (which are composed of one or two sentences), document length is not a major factor.</p>

<p>To illustrate BM25 scoring, consider these sentences, each having 10 words:</p>

<table>
    <tr>
        <td><span class="table-first-column">Query</span></td>
        <td><span class="sentence">I am taking my two daughters to school this afternoon.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Document 1</span></td>
        <td><span class="sentence">She is driving her three sons to class this morning.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Document 2</span></td>
        <td><span class="sentence">I am attending two afternoon courses at this arts school.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Document 3</span></td>
        <td><span class="sentence">He has not been playing basketball for a long time.</span></td>
    </tr>
</table>

<p>We use the <a href="https://github.com/dorianbrown/rank_bm25">Rank-BM25 package</a> to compute the BM25 scores of documents 1, 2 and 3 with respect to the query.
Note that this package uses the original Okapi BM25 formula for \(\textrm{idf}_{t}\).</p>

<pre>
# k1=1.5, b=0.75, epsilon=0.25
# idf = math.log[(N - df + 0.5)/(df + 0.5)]
# if idf of term is negative, use idf = epsilon*(average idf of entire corpus)

from rank_bm25 import BM25Okapi
corpus = [
    "She is driving her three sons to class this morning.",
    "I am attending two afternoon courses at this arts school.",
    "He has not been playing basketball for a long time."
]
tokenized_corpus = [doc.split(" ") for doc in corpus]
bm25 = BM25Okapi(tokenized_corpus)

query = "I am taking my two daughters to school this afternoon."
tokenized_query = query.split(" ")
doc_scores = bm25.get_scores(tokenized_query)
print(doc_scores)

[0.62972469 1.65137594 0.        ]
</pre>

<p>As expected, document 2 scores the highest (1.651) because it has six words in common with the query.
Document 3 scores zero because there are no common words with the query.
Although document 1 is most similar in syntax and semantics to the query, it scores very low (0.629) because "to" and "this" are the only common words.
</p>

<div id="tree"></div>

<hr>

<h5><strong>Syntactic similarity: Constituency parse trees</strong></h5>

<p>To represent the syntactic structure of a sentence, we use a constituency parse tree.
Using the <a href="https://stanfordnlp.github.io/CoreNLP/">Stanford CoreNLP</a> parser, we obtain the following parse trees, and the productions for each parse tree:</p>

<pre>
                          ROOT
                            |
                            S
  __________________________|___________________________________________
 |              VP                                                      |
 |    __________|___________                                            |
 |   |                      VP                                          |
 |   |     _________________|___________________________                |
 |   |    |          |                 PP               |               |
 |   |    |          |              ___|____            |               |
 NP  |    |          NP            |        NP        NP-TMP            |
 |   |    |      ____|______       |        |      _____|________       |
PRP VBP  VBG   PRP$  CD    NNS     TO       NN    DT             NN     .
 |   |    |     |    |      |      |        |     |              |      |
 I   am taking  my  two daughters  to     school this        afternoon  .

S -> NP VP .
NP -> PRP
VP -> VBP VP
VP -> VBG NP PP NP-TMP
NP -> PRP$ CD NNS
PP -> TO NP
NP -> NN
NP-TMP -> DT NN
</pre>

<pre>
                           ROOT
                            |
                            S
  __________________________|______________________________________
 |               VP                                                |
 |    ___________|__________                                       |
 |   |                      VP                                     |
 |   |      ________________|________________________              |
 |   |     |           |             PP              |             |
 |   |     |           |          ___|____           |             |
 NP  |     |           NP        |        NP       NP-TMP          |
 |   |     |      _____|____     |        |     _____|_______      |
PRP VBZ   VBG   PRP$   CD  NNS   TO       NN   DT            NN    .
 |   |     |     |     |    |    |        |    |             |     |
She  is driving her  three sons  to     class this        morning  .

S -> NP VP .
NP -> PRP
VP -> VBZ VP
VP -> VBG NP PP NP-TMP
NP -> PRP$ CD NNS
PP -> TO NP
NP -> NN
NP-TMP -> DT NN
</pre>

<pre>
                                  ROOT
                                   |
                                   S
  _________________________________|______________________________
 |                 VP                                             |
 |    _____________|_______________                               |
 |   |                             VP                             |
 |   |       ______________________|______________                |
 |   |      |             |                       PP              |
 |   |      |             |               ________|___            |
 NP  |      |             NP             |            NP          |
 |   |      |       ______|________      |    ________|_____      |
PRP VBP    VBG     CD     NN      NNS    IN  DT      NNS    NN    .
 |   |      |      |      |        |     |   |        |     |     |
 I   am attending two afternoon courses  at this     arts school  .

S -> NP VP .
NP -> PRP
VP -> VBP VP
VP -> VBG NP PP
NP -> CD NN NNS
PP -> IN NP
NP -> DT NNS NN
</pre>

<p>As expected, the constituency parse trees of the query and the first document have identical structures, while the parse tree of the second document is quite different.
Consequently, the query and the first document have almost the same set of productions (the only difference is <span class="pos-tag">VP -> VBP VP</span> for the query and <span class="pos-tag">VP -> VBZ VP</span> for the document).
For the second document, there are only two common productions (<span class="pos-tag">S -> NP VP .</span> and <span class="pos-tag">NP -> PRP</span>) with the query.</p>

This suggests that we can compute the syntactic similarities of two sentences by comparing their productions.
By concatenating the POS tags in a production using underscores (<span class="pos-tag">S -> NP VP</span> becomes <span class="pos-tag">S_NP_VP</span>),
we treat each production as a text string and use BM25 to compute the similarity score between the productions of query and document.

<hr>

<h5><strong>Syntactic similarity: Parts of speech</strong></h5>

<p>We can also use sequences of parts of speech (POS) tags to represent syntactic structure. Compare the sequences of POS tags for the following query and documents:</p>

<div class="pos-table">
<table>
    <tr>
        <td><span class="table-first-column">Query</span></td>
        <td><span class="pos">I am taking my two daughters to school this afternoon.</span></td>
        <td><span class="pos-tag">PRP VBP VBG PRP$ CD NNS TO NN DT NN</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Doc 1</span></td>
        <td><span class="pos">She is driving her three sons to class this morning.</span></td>
        <td><span class="pos-tag">PRP VBZ VBG PRP$ CD NNS TO NN DT NN</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Doc 2</span></td>
        <td><span class="pos">I am attending two afternoon courses at this arts school.</span></td>
        <td><span class="pos-tag">PRP VBP VBG CD NN NNS IN DT NNS NN</span></td>
    </tr>
</table>
</div>

<p>The POS tag sequences for the query and document 1 are almost identical.
For document 2, the first three words ("I am attending") have the same verb tense as the first three words of the query ("I am taking").
They are both present continuous tenses and thus have the same POS tag sequence <span class="pos-tag">PRP VBP VBG</span>.
</p>

<p>
Given a query and document, we can measure their syntactic similarity by comparing n-grams of POS tags.
By concatenating bigrams and trigrams of POS tags (<span class="pos-tag">PRP VBP VBG</span> becomes bigrams <span class="pos-tag">PRP_VBP</span> and <span class="pos-tag">VBP_VBG</span> and trigram <span class="pos-tag">PRP_VBP_VBG</span>),
we treat each n-gram as a text string and use BM25 to compute the similarity score between a query and a document.</p>

<hr>
<div id="feature"></div>

<h4><strong>3. Feature engineering and extraction</strong></h4>
<h5><strong>Feature engineering</strong></h5>

<p>We extract features from the raw sentences. The choice of features are based on these observations:</p>

<p><strong>The first word of the answer is a predictor of the grammar type.</strong> For verb tenses, the first word of the answer is all you need to predict whether it is a present (e.g. "is", "has"), past (e.g. "was", "had") or future (e.g. "will") tense,
and whether it is a continuous (e.g. "is", "was") or perfect (e.g. "has", "had") tense. For a two-word answer, if the first word is a verb, the answer is likely to be a phrasal verb.</p>

<p><strong>The last word of the answer is a predictor of the grammar type.</strong> If the last word of the answer is a verb, the answer is likely to be a verb tense. The part of speech of the verb is a good predictor of the tense.
For a multi-word answer, if the last word is a preposition, the answer is likely to be a phrasal verb.</p>

<p><strong>The length of the answer is a predictor of the grammar type.</strong> Almost all pronouns are single words, while verb tenses have two or three words depending on the tense.
Most phrasal verbs have two or three words,
but very often, phrasal verbs are used with a noun separating them (e.g. drop <strong>me</strong> off, break <strong>it</strong> down).</p>

<p><strong>The word before or after the answer may be related to the answer.</strong> The word before a verb tense is almost always a noun, and very often a personal pronoun. Conversely, the next word after a personal pronoun is likely to be a verb. </p>

<p><strong>Some answers tend to be at the start or end of a sentence.</strong> Reflexive pronouns (e.g. himself, yourself) tend to be at the end of a sentence.
Some prepositions and conjunctions, such as "if" and "according to", are often at the start of a sentence.</p>

<p><strong>Phrases with similar syntax have the same parts of speech.</strong> The syntactic similarity of two sentences can be measured by the similarity of parts of speech of the phrases in the sentences (e.g. "walked to school" and "drove to office").
Besides comparing the parts of speech of individual words in the query and document, we compare bigrams and trigrams of POS tags.
</p>

<p><strong>Sentences with similar syntax have common parse tree productions.</strong> As illustrated in the <a href="#tree">example above</a>, the parse tree productions are a good indicator of syntactic similarity.</p>

<div style="margin-bottom:30px;"></div>
<hr>

<h5><strong>Fields</strong></h5>

<p>Much of the learning-to-rank methods and algorithms that are widely used today originated from Microsoft,
whose researchers created the benchmark <a href="https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/#!letor-4-0">LETOR datasets</a>
and the <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf">LambdaMART</a> algorithm which we use to train our models.</p>

<p>For <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/letor3.pdf">LETOR 3.0</a> and <a href="https://arxiv.org/pdf/1306.2597.pdf">LETOR 4.0</a>,
the researchers divided a document into five fields: body, anchor, title, URL and the whole document (i.e. union of body, anchor, title and URL). For every field, they computed a set of measures. For example, for the document title, they computed
the term frequency TF, inverse document frequency IDF, TF-IDF, BM25, length, and language model measures. In total, there are 64 features in LETOR 3.0. and 46 features in LETOR 4.0.</p>

<p>We use a similar approach by breaking up a sentence into fields.
Consider the sentence:

<p><span class="example" style="margin-left:30px;">The company is expanding fast and <span class="answer">has opened</span> an office in India.</span></p>
<p>For Model 1, the entire sentence is the query input, so there is only one field:</p>
<table>
    <tr>
        <th><strong>Field</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="sentence">The company is expanding fast and has opened an office in India.</span></td>
    </tr>
</table>

<p>For Model 2, answer is a query input, and we define four fields:</p>
<table>
    <tr>
        <th><strong>Field</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="sentence">is expanding fast and has opened an office in India</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Words before answer</span></td>
        <td><span class="sentence">is expanding fast and</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Words after answer</span></td>
        <td><span class="sentence">an office in India</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="sentence">has opened</span></td>
    </tr>
</table>

<div style="margin-bottom:30px;"></div>

<hr>

<h5><strong>Features</strong></h5>

<p>
We extract features from each field. For bigrams and trigrams, we concatenate the POS tags with underscores.
Likewise for the parse tree productions, we removed the <span class="pos-tag">-></span> and concatenated the POS tags.
</p>

<p>
For the <strong>sentence</strong> field in Model 1, the features are:
</p>

<table>
    <tr>
        <th><strong>Feature</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Words (Solr default BM25)</span></td>
        <td><span class="pos-tag">The company is expanding fast and has opened an office in India.</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS tags</td>
        <td><span class="pos-tag">DT NN VBZ VBG RB CC VBZ VBN DT NN IN NNP</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS bigrams</td>
        <td><div class="pos-tag"><span class="block">DT_NN NN_VBZ VBZ_VBG VBG_RB</span>
        <span class="block">RB_CC CC_VBZ VBZ_VBN VBN_DT</span>
        <span class="block">DT_NN NN_IN IN_NNP</span></div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="pos-tag"><span class="block">DT_NN_VBZ NN_VBZ_VBG VBZ_VBG_RB VBG_RB_CC</span>
        <span class="block">RB_CC_VBZ CC_VBZ_VBN VBZ_VBN_DT VBN_DT_NN</span>
        <span class="block">DT_NN_IN NN_IN_NNP</span></div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><div class="pos-tag"><span class="block">S_NP_VP_. NP_DT_NN VP_VP_CC_VP VP_VBZ_VP</span>
        <span class="block">VP_VBG_ADVP ADVP_RB VP_VBZ_VP VP_VBN_NP</span>
        <span class="block">NP_NP_PP NP_DT_NN PP_IN_NP NP_NNP</span></div></td>
    </tr>
</table>

<p>Here is the parse tree for reference:</p>
<pre>
                                           ROOT
                                            |
                                            S
      ______________________________________|____________________________________________
     |                                      VP                                           |
     |                   ___________________|___________                                 |
     |                  |               |               VP                               |
     |                  |               |    ___________|________                        |
     |                  |               |   |                    VP                      |
     |                  |               |   |      ______________|_____                  |
     |                  VP              |   |     |                    NP                |
     |            ______|______         |   |     |          __________|_______          |
     |           |             VP       |   |     |         |                  PP        |
     |           |       ______|___     |   |     |         |               ___|____     |
     NP          |      |         ADVP  |   |     |         NP             |        NP   |
  ___|_____      |      |          |    |   |     |      ___|____          |        |    |
 DT        NN   VBZ    VBG         RB   CC VBZ   VBN    DT       NN        IN      NNP   .
 |         |     |      |          |    |   |     |     |        |         |        |    |
The     company  is expanding     fast and has  opened  an     office      in     India  .
</pre>

<div style="margin-bottom:30px;"></div>

<p>
There are four fields for Model 2. For the <strong>substring</strong> field, the features are:
</p>

<table>
    <tr>
        <th><strong>Feature</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Words (Solr default BM25)</span></td>
        <td><span class="pos-tag">is expanding fast and has opened an office in India</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS</td>
        <td><span class="pos-tag">VBZ VBG RB CC VBZ VBN DT NN IN NNP</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS bigrams</td>
        <td><div class="pos-tag"><span class="block">VBZ_VBG VBG_RB RB_CC CC_VBZ</span>
        <span class="block">VBZ_VBN VBN_DT DT_NN NN_IN</span>
        <span class="block">IN_NNP</span></div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="pos-tag"><span class="block">VBZ_VBG_RB VBG_RB_CC RB_CC_VBZ CC_VBZ_VBN</span>
        <span class="block">VBZ_VBN_DT VBN_DT_NN DT_NN_IN NN_IN_NNP</span></div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><div class="pos-tag"><span class="block">SINV_VP_NP VP_VP_CC_VP VP_VBZ_VP VP_VBG_ADVP</span>
        <span class="block">ADVP_RB VP_VBZ_VP VP_VBN NP_NP_PP</span>
        <span class="block">NP_DT_NN PP_IN_NP NP_NNP</span></div></td>
    </tr>
</table>

<p>Here is the parse tree for reference:</p>
<pre>
                           ROOT
                            |
                           SINV
                    ________|______________________________
                   VP                                      |
         __________|_____________                          |
        VP              |        |                         NP
  ______|______         |        |               __________|_______
 |             VP       |        VP             |                  PP
 |       ______|___     |    ____|____          |               ___|____
 |      |         ADVP  |   |         VP        NP             |        NP
 |      |          |    |   |         |      ___|____          |        |
VBZ    VBG         RB   CC VBZ       VBN    DT       NN        IN      NNP
 |      |          |    |   |         |     |        |         |        |
 is expanding     fast and has      opened  an     office      in     India
</pre>

<div style="margin-bottom:30px;"></div>

<p>
For the <strong>words before answer</strong> field in model 2, the features are:
</p>

<table>
    <tr>
        <th><strong>Feature</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="pos-tag">is expanding fast and</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Last word</span></td>
        <td><span class="pos-tag">and</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS (last word)</td>
        <td><span class="pos-tag">CC</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS</td>
        <td><span class="pos-tag">VBZ VBG RB CC</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS bigrams</td>
        <td><div class="pos-tag">VBZ_VBG VBG_RB RB_CC</div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="pos-tag">VBZ_VBG_RB VBG_RB_CC</div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><div class="pos-tag"><span class="block">FRAG_VP VP_VBZ_VP VP_VBG_ADVP_ADVP ADVP_RB ADVP_CC</span></div></td>
    </tr>
</table>

<p>Here is the parse tree for reference:</p>
<pre>
              ROOT
               |
              FRAG
               |
               VP
  _____________|____
 |                  VP
 |       ___________|____
 |      |          ADVP ADVP
 |      |           |    |
VBZ    VBG          RB   CC
 |      |           |    |
 is expanding      fast and
</pre>

<div style="margin-bottom:30px;"></div>

<p>
For the <strong>words after answer</strong> field in model 2, the features are:
</p>

<table>
    <tr>
        <th><strong>Feature</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="pos-tag">an office in India</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">First word</span></td>
        <td><span class="pos-tag">an</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS (first word)</td>
        <td><span class="pos-tag">DT</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS</td>
        <td><span class="pos-tag">DT NN IN NNP</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS bigrams</td>
        <td><div class="pos-tag">DT_NN NN_IN IN_NNP</div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="pos-tag">DT_NN_IN NN_IN_NNP</div></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><div class="pos-tag"><span class="block">NP_NP_PP NP_DT_NN PP_IN_NP NP_NNP</span></div></td>
    </tr>
</table>


<p>Here is the parse tree for reference:</p>
<pre>
               ROOT
                |
                NP
      __________|________
     |                   PP
     |                ___|____
     NP              |        NP
  ___|____           |        |
 DT       NN         IN      NNP
 |        |          |        |
 an     office       in     India
</pre>


<div style="margin-bottom:30px;"></div>


<p>
For the <strong>answer</strong> field in model 2, the features are:
</p>

<table>
    <tr>
        <th><strong>Feature</strong></th>
        <th><strong>Example</strong></th>
    </tr>
    <tr>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="pos-tag">has opened</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">First word</td>
        <td><span class="pos-tag">has</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Last word</td>
        <td><span class="pos-tag">opened</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS concatenated</span></td>
        <td><span class="pos-tag">VBZ_VBZ</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS (first word)</span></td>
        <td><span class="pos-tag">VBZ</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">POS (last word)</span></td>
        <td><span class="pos-tag">VBN</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Is first word of sentence</td>
        <td><span class="pos-tag">false</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Is last word of sentence</td>
        <td><span class="pos-tag">false</span></td>
    </tr>
    <tr>
        <td><span class="table-first-column">Length (number of words)</td>
        <td><span class="pos-tag">2</span></td>
    </tr>
</table>

<div id="value" style="margin-bottom:30px;"></div>
<div id="feature-value"></div>
<hr>

<h5><strong>Feature values</strong></h5>

<p>We extract the features from each raw sentence and save the values (i.e. words, POS tags, etc.) as a feature dataset.
We will compare these feature values between a query and document and compute similarity scores.
These scores are the final feature values for the learning algorithm.
The process of computing the final feature values is executed by Solr and is described in the <a href="#dataset">next section</a></p>

<p>For the LETOR datasets, the researchers used three classes of features:
features dependent on both query and document (e.g. BM25 score of title), features dependent on only the document (e.g. length of document title), and features dependent on only the query (e.g. inverse document frequency).</p>

<p>All our features are dependent on both query and document.
The final feature values are either BM25 scores (to compute the query-document similarity for the words, POS tags or parse tree productions) or a binary score (1 if query and document match, 0 if they don't match).
The following tables show the similarity measure used for each feature.</p>


<p><strong>Model 1:</strong></p>

<table>
    <tr>
        <th></th>
        <th><strong>Field</strong></th>
        <th><strong>Feature</strong></th>
        <th><strong>Value</strong></th>
    </tr>
    <tr>
        <td>1</td>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="">BM25 (Solr default score)</span></td>
    </tr>
    <tr>
        <td>2</td>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="table-first-column">POS words</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>3</td>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="table-first-column">POS bigrams</td>
        <td><span class="">BM25</span>
    </tr>
    <tr>
        <td>4</td>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="table-first-column">POS trigrams</td>
        <td><span class="">BM25</span>
    </tr>
    <tr>
        <td>5</td>
        <td><span class="table-first-column">Sentence</span></td>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><span class="">BM25</span>
    </tr>
</table>

<div style="margin-bottom:30px;"></div>

<p><strong>Model 2:</strong></p>

<table>
    <tr>
        <td></td>
        <th><strong>Field</strong></th>
        <th><strong>Feature</strong></th>
        <th><strong>Value</strong></th>
    </tr>
    <tr>
        <td>1</td>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="">BM25 (Solr default score)</span></td>
    </tr>
    <tr>
        <td>2</td>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="table-first-column">POS words</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>3</td>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="table-first-column">POS bigrams</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>4</td>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="table-first-column">POS trigrams</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>5</td>
        <td><span class="table-first-column">Substring</span></td>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>6</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>7</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">Last word</span></td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>8</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">POS last word</td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>9</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">POS words</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>10</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">POS bigrams</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>11</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="">BM25</div></td>
    </tr>
    <tr>
        <td>12</td>
        <td><span class="table-first-column">4 words before ans</span></td>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>13</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>14</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">First word</span></td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>15</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">POS first word</td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>16</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">POS words</td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>17</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">POS bigrams</td>
        <td><div class="">BM25</div></td>
    </tr>
    <tr>
        <td>18</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">POS trigrams</td>
        <td><div class="">BM25</div></td>
    </tr>
    <tr>
        <td>19</td>
        <td><span class="table-first-column">4 words after ans</span></td>
        <td><span class="table-first-column">Parse tree productions</td>
        <td><div class="">BM25<span class="block"></span></div></td>
    </tr>
    <tr>
        <td>20</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">Words</span></td>
        <td><span class="">BM25</span></td>
    </tr>
    <tr>
        <td>21</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">First word</td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>22</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">Last word</td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>23</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">POS concatenated</span></td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>24</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">POS first word</span></td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>25</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">POS last word</span></td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>26</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">Is ans first word of sentence</td>
        <td><span class="">1 if first word in query & doc, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>27</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">Is ans last word of sentence</td>
        <td><span class="">1 if last word in query & doc, 0 otherwise</span></td>
    </tr>
    <tr>
        <td>28</td>
        <td><span class="table-first-column">Answer</span></td>
        <td><span class="table-first-column">Answer length</td>
        <td><span class="">1 if query-doc match, 0 otherwise</span></td>
    </tr>
</table>

<div id="dataset"></div>
<hr>

<h4><strong>4. Learning-to-rank datasets</strong></h4>

<p>We describe the process of creating the training, validation and testing datasets.
We shall call these the <strong>learning-to-rank (LTR) datasets</strong>.</p>

<p>From the 1002 raw sentences, 152 are selected as queries and the remaining 850 as documents.
The 152 queries are manually selected to ensure that there are at least one sentence for every <a href="https://ronkow.com/search/data/#topic">preposition, conjunction, phrasal verb, verb tense and pronoun</a> that we cover.
For example, of the six sentences in the raw dataset for the pronoun "them", we select one sentence as the query.</p>

<p>We have chosen the learning algorithm <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf">LambdaMART</a>.
This algorithm takes in a LTR dataset in the <a href="https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/#!letor-4-0">LETOR dataset</a> format.
Each instance in a LETOR-format dataset is a query-document pair and all the feature values for this pair.
All feature values are dependent on both query and document.
We <a href="#value">compute feature values</a> as either BM25 scores or binary scores (representing a query-document match or no match).</p>


<h5><strong>Solr feature and model specifications</strong></h5>

<p>One of the great things about Solr is that it can compute these feature values for us.
First, we upload the features data for the 850 documents (from the <a href="#value">feature dataset</a> created earlier) to Solr.
Solr will index them to enable fast querying.</p>

<p>Next, we create a feature specification and a model specification (both in JSON format) and upload these specifications to Solr.
These two files tell Solr everything it needs to know to compute feature values.
They include information such as feature names and model name, and the instructions to Solr on what feature values to compute.
</p>

<p>
In Solr documentation, creating the feature specification is called feature engineering.
And I found this the trickiest part of the entire process because of the options Solr provides.
Each instruction in the feature specification is coded in Solr's query syntax,
which is dependent on the <a href="https://lucene.apache.org/solr/guide/6_6/query-syntax-and-parsing.html">query parser</a> chosen.
Solr provides many different query parsers,
each with many different ways of coding a query.
To add to the complexity, the query syntax for different parsers are not quite the same.
Eventually, I settled on using the <a href="https://lucene.apache.org/solr/guide/6_6/the-dismax-query-parser.html#the-dismax-query-parser">DisMax query parser</a> to compute BM25 scores,
and the <a href="https://lucene.apache.org/solr/guide/6_6/other-parsers.html#OtherParsers-TermQueryParser">Term query parser</a> to compute binary scores.</p>

<p>Once the specifications are correctly coded and uploaded, Solr is ready to compute feature values for a query input.
In Solr documentation, this process is called feature extraction.
</p>


<h5><strong>Solr feature value computation</strong></h5>

<p>We input one query instance at a time into Solr,
and Solr computes (according to the feature specification) and returns the feature values for this query with respect to each document.
This process of feeding queries one at a time to Solr is similar to performing a query in Solr.
We tell Solr to rank the documents by a particular score and return the top N documents, along with their feature values with respect to the particular query.
Typically, we want Solr to rank them by the original BM25 scores.
</p>

<div id="relevance"></div>

<h5><strong>Training, validation and testing sets</strong></h5>
<p>
We set N to be 50, and Solr will return at most 50 documents for each query.
(It is possible for Solr to return less than 50 documents, if there are less than 50 matching documents.)
Since we have 152 queries, there are 152 &times; 50 = 7600 query-document records in our LTR datasets.
We split the 152 queries into three datasets as follows:</p>

<table>
    <tr>
        <th><strong>Dataset</strong></th>
        <th><strong>Queries</strong></th>
        <th><strong>Query-document records</strong></th>

    </tr>
    <tr>
        <td class="table-first-column">Training</td>
        <td class="td-number">120</td>
        <td class="td-number">5500</td>
    </tr>
    <tr>
        <td class="table-first-column">Validation</td>
        <td class="td-number">20</td>
        <td class="td-number">1000</td>

    </tr>
    <tr>
        <td class="table-first-column">Testing</td>
        <td class="td-number">22</td>
        <td class="td-number">1100</td>
    </tr>
</table>


<h5><strong>Relevance scores</strong></h5>

<p>At this point, the only value that we have not yet computed is the relevance score.
For each query-document pair, we judge the relevance of the document according to a pre-defined criteria.
A relevance score is set if certain query-document feature values indicate similarity.</p>


<p>
For Model 1, we define two possible relevance values: 0 or 1.
1 indicates some syntactic similarity, 0 indicates no syntactic similarity.
Our criteria are:
</p>
<table>
    <tr>
        <th><strong>Score</strong></th>
        <th><strong>Criteria</strong></th>
    </tr>
    <tr>
        <td class="table-first-column td-number">1</td>
        <td><span class="display-block">The values of these features are all at least 1:</span>
        <span class="display-block">POS words,</span>
        <span class="display-block">POS bigrams,</span>
        <span class="display-block">POS trigrams,</span>
        <span class="display-block">Parse tree productions</span></td>
    </tr>
    <tr>
        <td class="table-first-column td-number">0</td>
        <td><span class="">Otherwise</span>
    </tr>
</table>

<p>
For Model 2, we define four possible relevance values: 0, 1, 2, 3.
The measure the similarity in topic and syntax of answers.
0 indicates that the document is least irrelevant to the query while 3 indicates greatest relevance.
Our criteria are:
</p>

<table>
    <tr>
        <th><strong>Score</strong></th>
        <th><strong>Criteria</strong></th>
    </tr>
    <tr>
        <td class="table-first-column td-number">3</td>
        <td><span class="display-block">Same topic id and values of these features are all 1:</span>
        <span class="display-block">POS concatenated (answer),</span>
        <span class="display-block">Last word (answer),</span>
        <span class="display-block">First word (answer),</span>
        <span class="display-block">POS (first word of ans),</span>
        <span class="display-block">POS (last word of ans),</span>
        <span class="display-block">Answer length</span></td>
    </tr>
    <tr>
        <td class="table-first-column td-number">2</td>
        <td><span class="display-block">Same topic id and values of these features are all 1:</span>

        <span class="display-block">First word (answer),</span>
        <span class="display-block">POS (first word of ans),</span>
        <span class="display-block">POS (last word of ans),</span>
        <span class="display-block">Answer length</span></td>
    </tr>
    <tr>
        <td class="table-first-column td-number">1</td>
        <td><span class="display-block">Same topic id and values of these features are all 1:</span>

        <span class="display-block">POS (first word of ans),</span>
        <span class="display-block">POS (last word of ans),</span>
        <span class="display-block">Answer length</span></td>
    </tr>
    <tr>
        <td class="table-first-column td-number">0</td>
        <td><span class="">Otherwise</span>
    </tr>
</table>


<h5><strong>Complete LTR dataset</strong></h5>

<p>A query-document instance in the LTR dataset in LETOR format is shown below.
The first column is the relevance score (2 in this example), qid is the query id, and the next 28 columns are the values of 27 features.
The document id is appended as a comment. </p>
<pre>
2 qid:1 1:10.519567 2:2.117584 3:4.3453412 4:3.3429412 5:5.3073893 6:9.60711 7:1.0 8:0.0 9:1.3453412
10:5.3453412 11:7.3453412 12:3.2718277 13:5.3581805 14:1.0 15:1.0 16:2.4753412 17:7.3453412
18:2.3453122 19:4.3451312 20:2.3411412 21:1.0 22:1.0 23:1.0 24:1.0 25:1.0 26:1.0 27:1.0 28:1.0 # docid:1
</pre>

The LTR datasets are now ready for LambdaMART.

<div style="margin-bottom:80px"></div>

{% endblock content %}

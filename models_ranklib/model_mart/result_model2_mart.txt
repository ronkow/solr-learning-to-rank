NOTE
Two test results are shown here, for two different metrics during training:
1. NDCG@10 (result: 0.9703)
2. ERR@10 (result: 0.9703) (RankLib's default metric)


java -jar RankLib-2.14.jar -train ../data/final/model2_train.txt -test ../data/final/model2_test.txt -validate ../data/final/model2_validate.txt -ranker 0 -metric2t NDCG@10 -metric2T NDCG@10 -save ../model_mart/model2_mart.txt

Discard orig. features
Training data:	../data/final/model2_train.txt
Test data:	../data/final/model2_test.txt
Validation data:	../data/final/model2_validate.txt
Feature vector representation: Dense.
Ranking method:	MART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	NDCG@10
Feature normalization: No
Model file: ../model_mart/model2_mart.txt

[+] MART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/final/model2_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/final/model2_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/final/model2_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.9309    | 0.8995    | 
2       | 0.9325    | 0.8986    | 
3       | 0.9328    | 0.8987    | 
4       | 0.9329    | 0.8987    | 
5       | 0.9341    | 0.8991    | 
6       | 0.9355    | 0.9013    | 
7       | 0.9362    | 0.9041    | 
8       | 0.9432    | 0.913     | 
9       | 0.9437    | 0.9131    | 
10      | 0.9441    | 0.9132    | 
11      | 0.9441    | 0.9131    | 
12      | 0.9456    | 0.9137    | 
13      | 0.9459    | 0.9146    | 
14      | 0.9458    | 0.9136    | 
15      | 0.946     | 0.9141    | 
16      | 0.9462    | 0.9152    | 
17      | 0.9456    | 0.9147    | 
18      | 0.9456    | 0.9158    | 
19      | 0.9459    | 0.9157    | 
20      | 0.9465    | 0.9158    | 
21      | 0.9466    | 0.9164    | 
22      | 0.9469    | 0.9163    | 
23      | 0.9463    | 0.9152    | 
24      | 0.9464    | 0.9142    | 
25      | 0.9466    | 0.9147    | 
26      | 0.9464    | 0.9152    | 
27      | 0.9476    | 0.9159    | 
28      | 0.9473    | 0.9159    | 
29      | 0.9474    | 0.9166    | 
30      | 0.9477    | 0.9172    | 
31      | 0.9478    | 0.9172    | 
32      | 0.948     | 0.9172    | 
33      | 0.9479    | 0.9176    | 
34      | 0.9482    | 0.9175    | 
35      | 0.9482    | 0.9183    | 
36      | 0.9481    | 0.9196    | 
37      | 0.9482    | 0.9207    | 
38      | 0.9482    | 0.9207    | 
39      | 0.9482    | 0.9184    | 
40      | 0.9482    | 0.9188    | 
41      | 0.9489    | 0.9193    | 
42      | 0.9489    | 0.9199    | 
43      | 0.949     | 0.9198    | 
44      | 0.949     | 0.9197    | 
45      | 0.949     | 0.9195    | 
46      | 0.949     | 0.9197    | 
47      | 0.949     | 0.9197    | 
48      | 0.9491    | 0.9195    | 
49      | 0.9491    | 0.9195    | 
50      | 0.9491    | 0.9203    | 
51      | 0.9497    | 0.9229    | 
52      | 0.9497    | 0.9229    | 
53      | 0.9497    | 0.9229    | 
54      | 0.9497    | 0.9229    | 
55      | 0.9497    | 0.9229    | 
56      | 0.9497    | 0.9229    | 
57      | 0.9497    | 0.9229    | 
58      | 0.9497    | 0.9229    | 
59      | 0.9497    | 0.9229    | 
60      | 0.9497    | 0.9229    | 
61      | 0.9495    | 0.9218    | 
62      | 0.9497    | 0.9218    | 
63      | 0.9497    | 0.9199    | 
64      | 0.9497    | 0.9196    | 
65      | 0.9495    | 0.9199    | 
66      | 0.9495    | 0.9199    | 
67      | 0.9495    | 0.9199    | 
68      | 0.9495    | 0.9199    | 
69      | 0.9495    | 0.9199    | 
70      | 0.9495    | 0.92      | 
71      | 0.9495    | 0.92      | 
72      | 0.9495    | 0.9196    | 
73      | 0.9495    | 0.921     | 
74      | 0.9495    | 0.9196    | 
75      | 0.9498    | 0.92      | 
76      | 0.9503    | 0.92      | 
77      | 0.9503    | 0.92      | 
78      | 0.9503    | 0.92      | 
79      | 0.9504    | 0.9196    | 
80      | 0.9504    | 0.921     | 
81      | 0.9504    | 0.9215    | 
82      | 0.9503    | 0.9215    | 
83      | 0.9503    | 0.9215    | 
84      | 0.9503    | 0.9215    | 
85      | 0.9503    | 0.9215    | 
86      | 0.9503    | 0.9215    | 
87      | 0.9503    | 0.9215    | 
88      | 0.9503    | 0.9216    | 
89      | 0.9503    | 0.9216    | 
90      | 0.9503    | 0.9216    | 
91      | 0.9503    | 0.9216    | 
92      | 0.9503    | 0.9216    | 
93      | 0.9503    | 0.9216    | 
94      | 0.9503    | 0.9205    | 
95      | 0.9503    | 0.9205    | 
96      | 0.9503    | 0.9205    | 
97      | 0.9503    | 0.9205    | 
98      | 0.9503    | 0.9205    | 
99      | 0.9506    | 0.9222    | 
100     | 0.9506    | 0.9211    | 
101     | 0.9506    | 0.9211    | 
102     | 0.9506    | 0.9211    | 
103     | 0.9506    | 0.9211    | 
104     | 0.9506    | 0.9211    | 
105     | 0.9504    | 0.9192    | 
106     | 0.9504    | 0.9192    | 
107     | 0.9506    | 0.9206    | 
108     | 0.9506    | 0.9206    | 
109     | 0.9506    | 0.9206    | 
110     | 0.9506    | 0.9206    | 
111     | 0.9506    | 0.9207    | 
112     | 0.9506    | 0.9207    | 
113     | 0.9506    | 0.9207    | 
114     | 0.9506    | 0.9207    | 
115     | 0.9506    | 0.9207    | 
116     | 0.9506    | 0.9207    | 
117     | 0.9506    | 0.9207    | 
118     | 0.9506    | 0.9206    | 
119     | 0.9506    | 0.9207    | 
120     | 0.9506    | 0.9207    | 
121     | 0.9509    | 0.9207    | 
122     | 0.9509    | 0.9212    | 
123     | 0.9509    | 0.9212    | 
124     | 0.951     | 0.9212    | 
125     | 0.9509    | 0.9212    | 
126     | 0.9509    | 0.9212    | 
127     | 0.9509    | 0.9212    | 
128     | 0.9507    | 0.9207    | 
129     | 0.9507    | 0.9207    | 
130     | 0.9507    | 0.9207    | 
131     | 0.9508    | 0.9207    | 
132     | 0.9508    | 0.9207    | 
133     | 0.9508    | 0.9212    | 
134     | 0.9508    | 0.9212    | 
135     | 0.9508    | 0.9212    | 
136     | 0.9508    | 0.9212    | 
137     | 0.9508    | 0.9212    | 
138     | 0.9508    | 0.9212    | 
139     | 0.9508    | 0.9212    | 
140     | 0.9507    | 0.9212    | 
141     | 0.9507    | 0.9217    | 
142     | 0.9508    | 0.9217    | 
143     | 0.9508    | 0.9217    | 
144     | 0.9508    | 0.9217    | 
145     | 0.9508    | 0.9217    | 
146     | 0.9509    | 0.9217    | 
147     | 0.9509    | 0.9217    | 
148     | 0.9509    | 0.9217    | 
149     | 0.9509    | 0.9217    | 
150     | 0.9509    | 0.9217    | 
151     | 0.9509    | 0.9217    | 
152     | 0.9509    | 0.9217    | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.9497
NDCG@10 on validation data: 0.9229
---------------------------------
NDCG@10 on test data: 0.9703

Model saved to: ../model_mart/model2_mart.txt


java -jar RankLib-2.14.jar -train ../data/final/model2_train.txt -test ../data/final/model2_test.txt -validate ../data/final/model2_validate.txt -ranker 0 -metric2T NDCG@10

Discard orig. features
Training data:	../data/final/model2_train.txt
Test data:	../data/final/model2_test.txt
Validation data:	../data/final/model2_validate.txt
Feature vector representation: Dense.
Ranking method:	MART
Feature description file:	Unspecified. All features will be used.
Train metric:	ERR@10
Test metric:	NDCG@10
Highest relevance label (to compute ERR): 4
Feature normalization: No

[+] MART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/final/model2_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/final/model2_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/final/model2_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | ERR@10-T  | ERR@10-V  | 
---------------------------------
1       | 0.5463    | 0.5423    | 
2       | 0.5464    | 0.5419    | 
3       | 0.5464    | 0.5419    | 
4       | 0.5464    | 0.5419    | 
5       | 0.5467    | 0.5421    | 
6       | 0.5479    | 0.5427    | 
7       | 0.5479    | 0.5438    | 
8       | 0.5488    | 0.5449    | 
9       | 0.5488    | 0.5436    | 
10      | 0.5488    | 0.5437    | 
11      | 0.5488    | 0.5437    | 
12      | 0.5508    | 0.5437    | 
13      | 0.551     | 0.5452    | 
14      | 0.5511    | 0.5439    | 
15      | 0.5511    | 0.544     | 
16      | 0.5511    | 0.544     | 
17      | 0.551     | 0.544     | 
18      | 0.551     | 0.5453    | 
19      | 0.5511    | 0.5453    | 
20      | 0.5511    | 0.5453    | 
21      | 0.5511    | 0.5453    | 
22      | 0.5511    | 0.5453    | 
23      | 0.5511    | 0.544     | 
24      | 0.5511    | 0.5438    | 
25      | 0.5511    | 0.5439    | 
26      | 0.5511    | 0.544     | 
27      | 0.5511    | 0.544     | 
28      | 0.5511    | 0.544     | 
29      | 0.5511    | 0.5441    | 
30      | 0.5511    | 0.5441    | 
31      | 0.5511    | 0.5441    | 
32      | 0.5511    | 0.5441    | 
33      | 0.5511    | 0.5451    | 
34      | 0.5511    | 0.545     | 
35      | 0.5511    | 0.5451    | 
36      | 0.5511    | 0.5478    | 
37      | 0.5511    | 0.5491    | 
38      | 0.5511    | 0.5491    | 
39      | 0.5511    | 0.5461    | 
40      | 0.5511    | 0.5463    | 
41      | 0.5512    | 0.5467    | 
42      | 0.5512    | 0.5467    | 
43      | 0.5513    | 0.5467    | 
44      | 0.5513    | 0.5467    | 
45      | 0.5513    | 0.5465    | 
46      | 0.5513    | 0.5467    | 
47      | 0.5513    | 0.5467    | 
48      | 0.5513    | 0.5465    | 
49      | 0.5513    | 0.5465    | 
50      | 0.5513    | 0.5466    | 
51      | 0.5514    | 0.5506    | 
52      | 0.5514    | 0.5506    | 
53      | 0.5514    | 0.5506    | 
54      | 0.5514    | 0.5506    | 
55      | 0.5514    | 0.5506    | 
56      | 0.5514    | 0.5506    | 
57      | 0.5514    | 0.5506    | 
58      | 0.5514    | 0.5506    | 
59      | 0.5514    | 0.5506    | 
60      | 0.5514    | 0.5506    | 
61      | 0.5514    | 0.55      | 
62      | 0.5514    | 0.55      | 
63      | 0.5514    | 0.5471    | 
64      | 0.5514    | 0.5468    | 
65      | 0.5514    | 0.5471    | 
66      | 0.5514    | 0.5471    | 
67      | 0.5514    | 0.5471    | 
68      | 0.5514    | 0.5471    | 
69      | 0.5514    | 0.5471    | 
70      | 0.5514    | 0.5471    | 
71      | 0.5514    | 0.5471    | 
72      | 0.5514    | 0.5468    | 
73      | 0.5514    | 0.5497    | 
74      | 0.5514    | 0.5468    | 
75      | 0.5514    | 0.5471    | 
76      | 0.5514    | 0.5471    | 
77      | 0.5514    | 0.5471    | 
78      | 0.5514    | 0.5471    | 
79      | 0.5514    | 0.5468    | 
80      | 0.5514    | 0.5497    | 
81      | 0.5514    | 0.5497    | 
82      | 0.5514    | 0.5497    | 
83      | 0.5514    | 0.5497    | 
84      | 0.5514    | 0.5497    | 
85      | 0.5514    | 0.5497    | 
86      | 0.5514    | 0.5497    | 
87      | 0.5514    | 0.5497    | 
88      | 0.5514    | 0.5497    | 
89      | 0.5514    | 0.5497    | 
90      | 0.5514    | 0.5497    | 
91      | 0.5514    | 0.5497    | 
92      | 0.5514    | 0.5497    | 
93      | 0.5514    | 0.5497    | 
94      | 0.5514    | 0.5484    | 
95      | 0.5514    | 0.5484    | 
96      | 0.5514    | 0.5484    | 
97      | 0.5514    | 0.5484    | 
98      | 0.5514    | 0.5484    | 
99      | 0.5515    | 0.5502    | 
100     | 0.5515    | 0.549     | 
101     | 0.5515    | 0.549     | 
102     | 0.5515    | 0.549     | 
103     | 0.5515    | 0.549     | 
104     | 0.5515    | 0.549     | 
105     | 0.5515    | 0.5455    | 
106     | 0.5515    | 0.5455    | 
107     | 0.5515    | 0.5485    | 
108     | 0.5515    | 0.5485    | 
109     | 0.5515    | 0.5485    | 
110     | 0.5515    | 0.5485    | 
111     | 0.5515    | 0.5485    | 
112     | 0.5515    | 0.5485    | 
113     | 0.5515    | 0.5485    | 
114     | 0.5515    | 0.5485    | 
115     | 0.5515    | 0.5485    | 
116     | 0.5515    | 0.5485    | 
117     | 0.5515    | 0.5485    | 
118     | 0.5515    | 0.5485    | 
119     | 0.5515    | 0.5485    | 
120     | 0.5515    | 0.5485    | 
121     | 0.5515    | 0.5485    | 
122     | 0.5515    | 0.549     | 
123     | 0.5515    | 0.549     | 
124     | 0.5515    | 0.549     | 
125     | 0.5515    | 0.549     | 
126     | 0.5515    | 0.549     | 
127     | 0.5515    | 0.549     | 
128     | 0.5515    | 0.5485    | 
129     | 0.5515    | 0.5485    | 
130     | 0.5515    | 0.5485    | 
131     | 0.5515    | 0.5485    | 
132     | 0.5515    | 0.5485    | 
133     | 0.5515    | 0.549     | 
134     | 0.5515    | 0.549     | 
135     | 0.5515    | 0.549     | 
136     | 0.5515    | 0.549     | 
137     | 0.5515    | 0.549     | 
138     | 0.5515    | 0.549     | 
139     | 0.5515    | 0.549     | 
140     | 0.5515    | 0.549     | 
141     | 0.5515    | 0.549     | 
142     | 0.5515    | 0.549     | 
143     | 0.5515    | 0.549     | 
144     | 0.5515    | 0.549     | 
145     | 0.5515    | 0.549     | 
146     | 0.5515    | 0.549     | 
147     | 0.5515    | 0.549     | 
148     | 0.5515    | 0.549     | 
149     | 0.5515    | 0.549     | 
150     | 0.5515    | 0.549     | 
151     | 0.5515    | 0.549     | 
152     | 0.5515    | 0.549     | 
---------------------------------
Finished sucessfully.
ERR@10 on training data: 0.5514
ERR@10 on validation data: 0.5506
---------------------------------
NDCG@10 on test data: 0.9703


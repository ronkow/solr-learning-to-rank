NOTE
Two test results are shown here, for two different metrics during training:
1. NDCG@10 (result: 0.7539)
2. ERR@10 (result: 0.7456) (RankLib's default metric)



java -jar RankLib-2.14.jar -train ../data/final/model2_train.txt -test ../data/final/model2_test.txt -validate ../data/final/model2_validate.txt -ranker 1 -metric2t NDCG@10 -metric2T NDCG@10 -save ../model_ranknet/model2_ranknet.txt

Discard orig. features
Training data:	../data/final/model2_train.txt
Test data:	../data/final/model2_test.txt
Validation data:	../data/final/model2_validate.txt
Feature vector representation: Dense.
Ranking method:	RankNet
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	NDCG@10
Feature normalization: No
Model file: ../model_ranknet/model2_ranknet.txt

[+] RankNet's Parameters:
No. of epochs: 100
No. of hidden layers: 1
No. of hidden nodes per layer: 10
Learning rate: 5.0E-5

Reading feature file [../data/final/model2_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/final/model2_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/final/model2_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
-----------------------------------------
Training starts...
--------------------------------------------------
#epoch  | % mis-ordered  | NDCG@10-T | NDCG@10-V | 
        |   pairs        |           |           | 
--------------------------------------------------
1       | 0.0988         | 0.6265    | 0.6039    | 
2       | 0.0786         | 0.6905    | 0.6712    | 
3       | 0.0704         | 0.7192    | 0.6889    | 
4       | 0.0669         | 0.7336    | 0.7064    | 
5       | 0.0653         | 0.739     | 0.7072    | 
6       | 0.0654         | 0.7364    | 0.7074    | 
7       | 0.0672         | 0.7237    | 0.6978    | 
8       | 0.0703         | 0.7115    | 0.6909    | 
9       | 0.073          | 0.7055    | 0.6803    | 
10      | 0.0749         | 0.7022    | 0.671     | 
11      | 0.0761         | 0.6955    | 0.6674    | 
12      | 0.0767         | 0.6927    | 0.6637    | 
13      | 0.0771         | 0.6918    | 0.661     | 
14      | 0.0772         | 0.6905    | 0.6605    | 
15      | 0.0771         | 0.6911    | 0.6571    | 
16      | 0.0773         | 0.6912    | 0.6568    | 
17      | 0.0772         | 0.6929    | 0.6571    | 
18      | 0.0771         | 0.694     | 0.6562    | 
19      | 0.0771         | 0.693     | 0.6557    | 
20      | 0.0771         | 0.6941    | 0.6561    | 
21      | 0.0769         | 0.693     | 0.6542    | 
22      | 0.0768         | 0.6931    | 0.6549    | 
23      | 0.0767         | 0.6916    | 0.6506    | 
24      | 0.0766         | 0.6918    | 0.6508    | 
25      | 0.0766         | 0.6913    | 0.6509    | 
26      | 0.0764         | 0.6917    | 0.6509    | 
27      | 0.0763         | 0.6923    | 0.653     | 
28      | 0.0759         | 0.6947    | 0.6506    | 
29      | 0.0757         | 0.6952    | 0.6491    | 
30      | 0.0754         | 0.6959    | 0.6479    | 
31      | 0.0751         | 0.693     | 0.6497    | 
32      | 0.0746         | 0.6905    | 0.6454    | 
33      | 0.0746         | 0.6905    | 0.6458    | 
34      | 0.074          | 0.6873    | 0.6454    | 
35      | 0.0735         | 0.6863    | 0.6458    | 
36      | 0.0726         | 0.6862    | 0.643     | 
37      | 0.0717         | 0.6811    | 0.6405    | 
38      | 0.0714         | 0.6786    | 0.6398    | 
39      | 0.0704         | 0.6722    | 0.6402    | 
40      | 0.0705         | 0.6721    | 0.6416    | 
41      | 0.0686         | 0.6699    | 0.6469    | 
42      | 0.0683         | 0.668     | 0.6459    | 
43      | 0.0681         | 0.6682    | 0.6461    | 
44      | 0.0653         | 0.6673    | 0.6439    | 
45      | 0.0633         | 0.6671    | 0.6434    | 
46      | 0.0635         | 0.6682    | 0.6482    | 
47      | 0.0611         | 0.66      | 0.6495    | 
48      | 0.0596         | 0.6584    | 0.6498    | 
49      | 0.0582         | 0.6522    | 0.6529    | 
50      | 0.0555         | 0.649     | 0.6449    | 
51      | 0.054          | 0.6477    | 0.6412    | 
52      | 0.0535         | 0.6463    | 0.6402    | 
53      | 0.0512         | 0.6418    | 0.6359    | 
54      | 0.0514         | 0.6408    | 0.6367    | 
55      | 0.0497         | 0.6355    | 0.6263    | 
56      | 0.0482         | 0.6294    | 0.6189    | 
57      | 0.0439         | 0.6237    | 0.6108    | 
58      | 0.0439         | 0.6237    | 0.6113    | 
59      | 0.0446         | 0.6254    | 0.6124    | 
60      | 0.0403         | 0.6107    | 0.603     | 
61      | 0.0402         | 0.6095    | 0.6032    | 
62      | 0.0421         | 0.6168    | 0.6082    | 
63      | 0.0384         | 0.6024    | 0.5965    | 
64      | 0.0398         | 0.6101    | 0.6041    | 
65      | 0.0354         | 0.5955    | 0.5913    | 
66      | 0.0344         | 0.5874    | 0.5886    | 
67      | 0.0353         | 0.5962    | 0.5905    | 
68      | 0.0344         | 0.5891    | 0.5877    | 
69      | 0.0335         | 0.5871    | 0.5888    | 
70      | 0.0305         | 0.577     | 0.5784    | 
71      | 0.0317         | 0.5814    | 0.5831    | 
72      | 0.0289         | 0.5721    | 0.5707    | 
73      | 0.0288         | 0.5741    | 0.5715    | 
74      | 0.0268         | 0.5679    | 0.5679    | 
75      | 0.0256         | 0.568     | 0.567     | 
76      | 0.0248         | 0.5631    | 0.5663    | 
77      | 0.0256         | 0.568     | 0.5675    | 
78      | 0.0253         | 0.5671    | 0.5663    | 
79      | 0.0239         | 0.5614    | 0.5641    | 
80      | 0.0218         | 0.5558    | 0.5595    | 
81      | 0.0228         | 0.5578    | 0.5606    | 
82      | 0.0195         | 0.5529    | 0.5563    | 
83      | 0.0201         | 0.5545    | 0.558     | 
84      | 0.0197         | 0.5539    | 0.5564    | 
85      | 0.0196         | 0.5536    | 0.5564    | 
86      | 0.02           | 0.5547    | 0.5581    | 
87      | 0.0189         | 0.5513    | 0.5519    | 
88      | 0.0176         | 0.5485    | 0.5516    | 
89      | 0.0181         | 0.5482    | 0.5516    | 
90      | 0.0165         | 0.5411    | 0.5478    | 
91      | 0.0154         | 0.5378    | 0.5473    | 
92      | 0.0156         | 0.5387    | 0.5473    | 
93      | 0.0154         | 0.5371    | 0.5473    | 
94      | 0.0132         | 0.5346    | 0.5401    | 
95      | 0.0138         | 0.5354    | 0.543     | 
96      | 0.0116         | 0.5301    | 0.5374    | 
97      | 0.0114         | 0.5295    | 0.5353    | 
98      | 0.0118         | 0.5312    | 0.5397    | 
99      | 0.011          | 0.5291    | 0.5358    | 
100     | 0.0105         | 0.5285    | 0.5351    | 
--------------------------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.7364
NDCG@10 on validation data: 0.7074
---------------------------------
NDCG@10 on test data: 0.7539

Model saved to: ../model_ranknet/model2_ranknet.txt

------------------------------------------------------------------------
java -jar RankLib-2.14.jar -train ../data/final/model2_train.txt -test ../data/final/model2_test.txt -validate ../data/final/model2_validate.txt -ranker 1 -metric2T NDCG@10

Discard orig. features
Training data:	../data/final/model2_train.txt
Test data:	../data/final/model2_test.txt
Validation data:	../data/final/model2_validate.txt
Feature vector representation: Dense.
Ranking method:	RankNet
Feature description file:	Unspecified. All features will be used.
Train metric:	ERR@10
Test metric:	NDCG@10
Highest relevance label (to compute ERR): 4
Feature normalization: No

[+] RankNet's Parameters:
No. of epochs: 100
No. of hidden layers: 1
No. of hidden nodes per layer: 10
Learning rate: 5.0E-5

Reading feature file [../data/final/model2_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/final/model2_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/final/model2_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
-----------------------------------------
Training starts...
--------------------------------------------------
#epoch  | % mis-ordered  | ERR@10-T  | ERR@10-V  | 
        |   pairs        |           |           | 
--------------------------------------------------
1       | 0.1925         | 0.3225    | 0.3072    | 
2       | 0.1186         | 0.4063    | 0.4103    | 
3       | 0.0946         | 0.4309    | 0.457     | 
4       | 0.0835         | 0.4575    | 0.4662    | 
5       | 0.078          | 0.4668    | 0.4746    | 
6       | 0.0747         | 0.4687    | 0.4769    | 
7       | 0.0727         | 0.4721    | 0.4801    | 
8       | 0.0721         | 0.4724    | 0.4844    | 
9       | 0.0721         | 0.4698    | 0.4789    | 
10      | 0.0726         | 0.4649    | 0.4697    | 
11      | 0.0734         | 0.4572    | 0.4562    | 
12      | 0.0751         | 0.4395    | 0.4388    | 
13      | 0.0765         | 0.4316    | 0.4367    | 
14      | 0.078          | 0.4241    | 0.4402    | 
15      | 0.0785         | 0.4348    | 0.4439    | 
16      | 0.079          | 0.4394    | 0.4548    | 
17      | 0.0792         | 0.446     | 0.4533    | 
18      | 0.079          | 0.4499    | 0.4621    | 
19      | 0.079          | 0.451     | 0.4629    | 
20      | 0.0789         | 0.4513    | 0.4635    | 
21      | 0.0786         | 0.451     | 0.4629    | 
22      | 0.0782         | 0.4513    | 0.4627    | 
23      | 0.0778         | 0.4528    | 0.4614    | 
24      | 0.0776         | 0.4525    | 0.4611    | 
25      | 0.0776         | 0.4522    | 0.4603    | 
26      | 0.0773         | 0.4522    | 0.4603    | 
27      | 0.077          | 0.4522    | 0.4602    | 
28      | 0.0768         | 0.4524    | 0.4597    | 
29      | 0.0766         | 0.4515    | 0.46      | 
30      | 0.0763         | 0.4529    | 0.4563    | 
31      | 0.0763         | 0.4516    | 0.4522    | 
32      | 0.0761         | 0.4509    | 0.4523    | 
33      | 0.0758         | 0.4536    | 0.4522    | 
34      | 0.0756         | 0.4545    | 0.4548    | 
35      | 0.0755         | 0.4554    | 0.453     | 
36      | 0.0752         | 0.4584    | 0.452     | 
37      | 0.0746         | 0.4595    | 0.4476    | 
38      | 0.0743         | 0.4604    | 0.452     | 
39      | 0.0738         | 0.4566    | 0.4495    | 
40      | 0.0731         | 0.4546    | 0.4481    | 
41      | 0.0723         | 0.4516    | 0.4492    | 
42      | 0.0713         | 0.4513    | 0.4475    | 
43      | 0.0712         | 0.4546    | 0.4509    | 
44      | 0.0701         | 0.4519    | 0.446     | 
45      | 0.0695         | 0.4494    | 0.4457    | 
46      | 0.0688         | 0.4453    | 0.4448    | 
47      | 0.0679         | 0.4463    | 0.4449    | 
48      | 0.0668         | 0.44      | 0.4454    | 
49      | 0.0653         | 0.441     | 0.4469    | 
50      | 0.0656         | 0.4407    | 0.448     | 
51      | 0.0638         | 0.4436    | 0.4469    | 
52      | 0.0609         | 0.4421    | 0.4462    | 
53      | 0.061          | 0.4422    | 0.4466    | 
54      | 0.0589         | 0.4368    | 0.448     | 
55      | 0.0589         | 0.4372    | 0.4483    | 
56      | 0.0585         | 0.4372    | 0.4481    | 
57      | 0.0555         | 0.4323    | 0.4412    | 
58      | 0.053          | 0.4259    | 0.4393    | 
59      | 0.0544         | 0.4319    | 0.4407    | 
60      | 0.0504         | 0.4244    | 0.4398    | 
61      | 0.0512         | 0.4252    | 0.4401    | 
62      | 0.0472         | 0.4175    | 0.4317    | 
63      | 0.046          | 0.417     | 0.4316    | 
64      | 0.0458         | 0.4167    | 0.43      | 
65      | 0.0438         | 0.4165    | 0.4292    | 
66      | 0.0453         | 0.4168    | 0.4297    | 
67      | 0.0407         | 0.411     | 0.4273    | 
68      | 0.0398         | 0.4088    | 0.4272    | 
69      | 0.0391         | 0.4074    | 0.4255    | 
70      | 0.0394         | 0.4085    | 0.4273    | 
71      | 0.0378         | 0.4053    | 0.4252    | 
72      | 0.0365         | 0.4045    | 0.4244    | 
73      | 0.0358         | 0.4028    | 0.4229    | 
74      | 0.0347         | 0.4016    | 0.4221    | 
75      | 0.0352         | 0.4026    | 0.4228    | 
76      | 0.0334         | 0.4006    | 0.4223    | 
77      | 0.032          | 0.3971    | 0.4206    | 
78      | 0.0315         | 0.3968    | 0.42      | 
79      | 0.0319         | 0.3969    | 0.4212    | 
80      | 0.0295         | 0.3945    | 0.4183    | 
81      | 0.0284         | 0.3935    | 0.4177    | 
82      | 0.0275         | 0.393     | 0.4175    | 
83      | 0.0305         | 0.3955    | 0.4191    | 
84      | 0.0279         | 0.3934    | 0.4177    | 
85      | 0.0284         | 0.3938    | 0.4178    | 
86      | 0.025          | 0.3908    | 0.4169    | 
87      | 0.023          | 0.3894    | 0.4159    | 
88      | 0.0223         | 0.3888    | 0.4158    | 
89      | 0.0221         | 0.3888    | 0.4155    | 
90      | 0.0225         | 0.3888    | 0.4158    | 
91      | 0.0203         | 0.388     | 0.4151    | 
92      | 0.0196         | 0.3852    | 0.4102    | 
93      | 0.0215         | 0.3885    | 0.4155    | 
94      | 0.0208         | 0.388     | 0.415     | 
95      | 0.0202         | 0.3871    | 0.4134    | 
96      | 0.0178         | 0.3828    | 0.4082    | 
97      | 0.0207         | 0.3879    | 0.415     | 
98      | 0.0175         | 0.3828    | 0.4082    | 
99      | 0.017          | 0.3819    | 0.4076    | 
100     | 0.0172         | 0.3821    | 0.408     | 
--------------------------------------------------
Finished sucessfully.
ERR@10 on training data: 0.4724
ERR@10 on validation data: 0.4844
---------------------------------
NDCG@10 on test data: 0.7456


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random    # random.randint\n",
    "import os        # os.path.join\n",
    "import csv       # csv.DictReader\n",
    "import json      # json.dumps\n",
    "import requests  # requests.post\n",
    "import urllib    # urllib.parse.urlencode, urllib.parse.quote_plus\n",
    "import re        # re.match\n",
    "import solr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA PATHS, GLOBAL VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = \"data/\"\n",
    "\n",
    "FEATURE_QUERY_TRAIN = os.path.join(DATA_DIR, \"feature_query_train.csv\")\n",
    "FEATURE_QUERY_VALIDATE_TEST = os.path.join(DATA_DIR, \"feature_query_validate_test.csv\")\n",
    "\n",
    "FEATURE_FILE_TEMPLATE_MODEL1 = os.path.join(DATA_DIR, \"final/no_relevance_model1_{:s}.txt\")\n",
    "FEATURE_FILE_TEMPLATE_MODEL2 = os.path.join(DATA_DIR, \"final/no_relevance_model2_{:s}.txt\")\n",
    "\n",
    "\n",
    "FILE_MODEL1_NO_RELEVANCE_TRAIN    = os.path.join(DATA_DIR, \"final/no_relevance_model1_train.txt\")\n",
    "FILE_MODEL1_NO_RELEVANCE_VALIDATE = os.path.join(DATA_DIR, \"final/no_relevance_model1_validate.txt\")\n",
    "FILE_MODEL1_NO_RELEVANCE_TEST     = os.path.join(DATA_DIR, \"final/no_relevance_model1_test.txt\")\n",
    "\n",
    "FILE_MODEL2_NO_RELEVANCE_TRAIN    = os.path.join(DATA_DIR, \"final/no_relevance_model2_train.txt\")\n",
    "FILE_MODEL2_NO_RELEVANCE_VALIDATE = os.path.join(DATA_DIR, \"final/no_relevance_model2_validate.txt\")\n",
    "FILE_MODEL2_NO_RELEVANCE_TEST     = os.path.join(DATA_DIR, \"final/no_relevance_model2_test.txt\")\n",
    "\n",
    "FILE_MODEL1_TRAIN    = os.path.join(DATA_DIR, \"final/model1_train.txt\")\n",
    "FILE_MODEL1_VALIDATE = os.path.join(DATA_DIR, \"final/model1_validate.txt\")\n",
    "FILE_MODEL1_TEST     = os.path.join(DATA_DIR, \"final/model1_test.txt\")\n",
    "\n",
    "FILE_MODEL2_TRAIN    = os.path.join(DATA_DIR, \"final/model2_train.txt\")\n",
    "FILE_MODEL2_VALIDATE = os.path.join(DATA_DIR, \"final/model2_validate.txt\")\n",
    "FILE_MODEL2_TEST     = os.path.join(DATA_DIR, \"final/model2_test.txt\")\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/core1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CREATE DATASETS in LETOR FORMAT for MODELS 1 AND 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_letor(qid, feat_string, docid, FEATURE_LIST):\n",
    "    \"\"\"\n",
    "    arguments: query_id, feature string, doc_id, query text, relevance score \n",
    "    \"\"\"    \n",
    "    feat_pairs = []\n",
    "    feature_name_to_id = {name: idx + 1 for idx, name in enumerate(FEATURE_LIST)}\n",
    "    \n",
    "    for feat_nv in feat_string.split(','):\n",
    "        feat_name, feat_value = feat_nv.split('=')\n",
    "        \n",
    "        feat_id = str(feature_name_to_id[feat_name])\n",
    "               \n",
    "        feat_value = float(feat_value)\n",
    "        feat_value = str(feat_value)\n",
    "        \n",
    "        feat_pairs.append( ':'.join([feat_id, feat_value]) )\n",
    "\n",
    "    return f\"qid:{qid} {' '.join(feat_pairs)} # docid:{docid}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_lists():\n",
    "    \n",
    "    with open(FEATURE_QUERY_TRAIN) as f1:\n",
    "        QUERY_LIST_TRAIN = [ {k: v for k, v in row.items()} for row in csv.DictReader(f1, skipinitialspace=True) ]\n",
    "\n",
    "    with open(FEATURE_QUERY_VALIDATE_TEST) as f2:\n",
    "        QUERY_LIST_VALIDATE_TEST = [ {k: v for k, v in row.items()} for row in csv.DictReader(f2, skipinitialspace=True) ]\n",
    "    \n",
    "    random.seed(1)\n",
    "    random.shuffle(QUERY_LIST_VALIDATE_TEST)\n",
    "       \n",
    "    train_q    = QUERY_LIST_TRAIN\n",
    "    validate_q = QUERY_LIST_VALIDATE_TEST[0:75]\n",
    "    test_q     = QUERY_LIST_VALIDATE_TEST[75:]\n",
    "    \n",
    "    return train_q, validate_q, test_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_solr(payload):\n",
    "    \n",
    "    params = urllib.parse.urlencode(payload, quote_via=urllib.parse.quote_plus)\n",
    "    search_url = SOLR_URL + \"/query?\" + params\n",
    "    resp = requests.get(search_url)\n",
    "    resp_json = json.loads(resp.text)\n",
    "    \n",
    "    return resp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1_data(FEATURE_LIST, train_q, validate_q, test_q):\n",
    "    \n",
    "    feat_suffixes = [\"train\", \"validate\", \"test\"]\n",
    "\n",
    "    qid = 1\n",
    "    for i, queries in enumerate([train_q, validate_q, test_q]):    #[(0,'train_q'), (1,'validate_q'), (2,'test_q')]\n",
    "    \n",
    "        model_data_file = open(FEATURE_FILE_TEMPLATE_MODEL1.format(feat_suffixes[i]), \"w\")\n",
    "\n",
    "        for q in queries:\n",
    "            #print(f\"Extracting features for query: {q['qa']} ({feat_suffixes[i]})\")     \n",
    "            \n",
    "            f1 = q[\"qa\"]\n",
    "            f2 = q[\"qa_pos\"]\n",
    "            f3 = q[\"qa_pos_bigram\"]\n",
    "            f4 = q[\"qa_pos_trigram\"]\n",
    "            f5 = q[\"qa_parse_tree\"]\n",
    "                          \n",
    "            payload = {\n",
    "                \"q\": f1,\n",
    "                \"defType\": \"edismax\",\n",
    "                \"qf\": \"qa\",\n",
    "                \"rq\": f'{{!ltr model=linear_model1 \\\n",
    "                    efi.q2=\"{f2}\" \\\n",
    "                    efi.q3=\"{f3}\" \\\n",
    "                    efi.q4=\"{f4}\" \\\n",
    "                    efi.q5=\"{f5}\"}}',\n",
    "                \"fl\": \"id,[features]\",   \n",
    "                \"rows\": 50\n",
    "            }\n",
    "            \n",
    "            resp_json = query_solr(payload) \n",
    "            \n",
    "            for doc in resp_json[\"response\"][\"docs\"]:\n",
    "                docid = int(doc[\"id\"])            \n",
    "                feat_string = doc[\"[features]\"]\n",
    "                qid = int(q[\"id\"])\n",
    "                model_data_file.write( \"{:s}\\n\".format( format_letor(qid, feat_string, docid, FEATURE_LIST) ) )  ###\n",
    "            \n",
    "        model_data_file.close()\n",
    "\n",
    "    print(f\"MODEL 1: number of queries: train {len(train_q)}, validation {len(validate_q)}, test {len(test_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2_data(FEATURE_LIST, train_q, validate_q, test_q):\n",
    "           \n",
    "    feat_suffixes = [\"train\", \"validate\", \"test\"]\n",
    "\n",
    "    qid = 1\n",
    "    for i, queries in enumerate([train_q, validate_q, test_q]):   \n",
    "    \n",
    "        model_data_file = open(FEATURE_FILE_TEMPLATE_MODEL2.format(feat_suffixes[i]), \"w\")\n",
    "\n",
    "        for q in queries:\n",
    "            \n",
    "            f1 = q[\"ss\"]\n",
    "            f2 = q[\"ss_pos\"]\n",
    "            f3 = q[\"ss_pos_bigram\"]\n",
    "            f4 = q[\"ss_pos_trigram\"]\n",
    "            f5 = q[\"ss_parse_tree\"]\n",
    "            f6 = q[\"before\"]\n",
    "            f7 = q[\"before_last\"]\n",
    "            f8 = (q[\"before_last_pos\"]).lower()\n",
    "            f9 = q[\"before_pos\"]\n",
    "            f10 = q[\"before_pos_bigram\"]\n",
    "            f11 = q[\"before_pos_trigram\"]\n",
    "            f12 = q[\"before_parse_tree\"]\n",
    "            f13 = q[\"after\"]\n",
    "            f14 = q[\"after_first\"]\n",
    "            f15 = (q[\"after_first_pos\"]).lower()\n",
    "            f16 = q[\"after_pos\"]\n",
    "            f17 = q[\"after_pos_bigram\"]\n",
    "            f18 = q[\"after_pos_trigram\"]\n",
    "            f19 = q[\"after_parse_tree\"]\n",
    "            f20 = q[\"ans\"]\n",
    "            f21 = q[\"ans_first\"]\n",
    "            f22 = q[\"ans_last\"]\n",
    "            f23 = (q[\"ans_pos\"]).lower()\n",
    "            f24 = (q[\"ans_first_pos\"]).lower()\n",
    "            f25 = (q[\"ans_last_pos\"]).lower()\n",
    "            f26 = q[\"ans_is_first\"]\n",
    "            f27 = q[\"ans_is_last\"]\n",
    "            f28 = q[\"ans_length\"]\n",
    "            f_topic = str(q[\"qb_topic_id\"])\n",
    "            \n",
    "            payload = {\n",
    "                \"q\": f1,\n",
    "                \"defType\": \"edismax\",\n",
    "                \"qf\": \"ss\",\n",
    "                \"rq\": f'{{!ltr model=linear_model2 \\\n",
    "                    efi.q2=\"{f2}\" \\\n",
    "                    efi.q3=\"{f3}\" \\\n",
    "                    efi.q4=\"{f4}\" \\\n",
    "                    efi.q5=\"{f5}\" \\\n",
    "                    efi.q6=\"{f6}\" \\\n",
    "                    efi.q7=\"{f7}\" \\\n",
    "                    efi.q8=\"{f8}\" \\\n",
    "                    efi.q9=\"{f9}\" \\\n",
    "                    efi.q10=\"{f10}\" \\\n",
    "                    efi.q11=\"{f11}\" \\\n",
    "                    efi.q12=\"{f12}\" \\\n",
    "                    efi.q13=\"{f13}\" \\\n",
    "                    efi.q14=\"{f14}\" \\\n",
    "                    efi.q15=\"{f15}\" \\\n",
    "                    efi.q16=\"{f16}\" \\\n",
    "                    efi.q17=\"{f17}\" \\\n",
    "                    efi.q18=\"{f18}\" \\\n",
    "                    efi.q19=\"{f19}\" \\\n",
    "                    efi.q20=\"{f20}\" \\\n",
    "                    efi.q21=\"{f21}\" \\\n",
    "                    efi.q22=\"{f22}\" \\\n",
    "                    efi.q23=\"{f23}\" \\\n",
    "                    efi.q24=\"{f24}\" \\\n",
    "                    efi.q25=\"{f25}\" \\\n",
    "                    efi.q26=\"{f26}\" \\\n",
    "                    efi.q27=\"{f27}\" \\\n",
    "                    efi.q28=\"{f28}\" \\\n",
    "                    efi.q_topic=\"{f_topic}\"}}',\n",
    "                \"fl\": \"score,id,[features]\",   \n",
    "                \"rows\": 50\n",
    "            }\n",
    "            \n",
    "            resp_json = query_solr(payload) \n",
    "            \n",
    "            for doc in resp_json[\"response\"][\"docs\"]:\n",
    "                docid = int(doc[\"id\"])            \n",
    "                feat_string = doc[\"[features]\"]\n",
    "                qid = int(q[\"id\"])\n",
    "                model_data_file.write( \"{:s}\\n\".format( format_letor(qid, feat_string, docid, FEATURE_LIST) ) )  ###\n",
    "            \n",
    "        model_data_file.close()\n",
    "\n",
    "    print(f\"MODEL 2: number of queries: train {len(train_q)}, validation {len(validate_q)}, test {len(test_q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **COMPUTE and PREPEND RELEVANCE LABELS TO DATASETS** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type, model):\n",
    "    \n",
    "    if model==1:\n",
    "        if data_type == 'train':\n",
    "            FILE_READ = FILE_MODEL1_NO_RELEVANCE_TRAIN\n",
    "            FILE_WRITE = FILE_MODEL1_TRAIN\n",
    "        \n",
    "        elif data_type == 'validate':   \n",
    "            FILE_READ = FILE_MODEL1_NO_RELEVANCE_VALIDATE\n",
    "            FILE_WRITE = FILE_MODEL1_VALIDATE\n",
    "        \n",
    "        elif data_type == 'test':\n",
    "            FILE_READ = FILE_MODEL1_NO_RELEVANCE_TEST\n",
    "            FILE_WRITE = FILE_MODEL1_TEST\n",
    "\n",
    "    elif model==2:\n",
    "        if data_type == 'train':\n",
    "            FILE_READ = FILE_MODEL2_NO_RELEVANCE_TRAIN\n",
    "            FILE_WRITE = FILE_MODEL2_TRAIN\n",
    "        \n",
    "        elif data_type == 'validate':   \n",
    "            FILE_READ = FILE_MODEL2_NO_RELEVANCE_VALIDATE\n",
    "            FILE_WRITE = FILE_MODEL2_VALIDATE\n",
    "        \n",
    "        elif data_type == 'test':\n",
    "            FILE_READ = FILE_MODEL2_NO_RELEVANCE_TEST\n",
    "            FILE_WRITE = FILE_MODEL2_TEST\n",
    "\n",
    "    return FILE_READ, FILE_WRITE        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training, validation and test datasets created so far do not have the relevance label as the first item\n",
    "# We prepend this relevance label here\n",
    "\n",
    "def prepend_relevance_label_model_one(FILE_READ, FILE_WRITE):\n",
    "    \"\"\"\n",
    "    RELEVANCE = 1 if these feature values are >= 3.0:\n",
    "    qa_pos           (feature 2)  \n",
    "    qa_pos_bigram    (feature 3)     \n",
    "    qa_pos_trigram   (feature 4)\n",
    "    qa_parse_tree    (feature 5)    \n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    with open(FILE_READ, 'r') as fread:\n",
    "        for line in fread:\n",
    "            data_list.append(line)\n",
    "\n",
    "    for i, record in enumerate(data_list):\n",
    "        record_list = record.split()\n",
    "        match=0\n",
    "        score=0\n",
    "        \n",
    "        for k, feature in enumerate(record_list):\n",
    "    \n",
    "            if k==2 or k==3 or k==4 or k==5:\n",
    "                pattern1 = fr'^{k}:0\\.'\n",
    "                pattern2 = fr'^{k}:1\\.'\n",
    "                pattern3 = fr'^{k}:2\\.'\n",
    "                \n",
    "                match1 = re.match(pattern1,feature)\n",
    "                match2 = re.match(pattern2,feature)\n",
    "                match3 = re.match(pattern3,feature)\n",
    "                \n",
    "                if not match1 and not match2 and not match3:\n",
    "                    match = 1\n",
    "                else:\n",
    "                    match = 0\n",
    "                    \n",
    "                score += match\n",
    "                    \n",
    "        if score==4:\n",
    "            relevance = 1\n",
    "        else:\n",
    "            relevance = 0\n",
    "        \n",
    "        data_list[i] = str(relevance) + ' ' + record\n",
    "\n",
    "    with open(FILE_WRITE, 'w') as fwrite:\n",
    "        for x in data_list:\n",
    "            fwrite.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_relevance_label_model_two(FILE_READ, FILE_WRITE):\n",
    "    \"\"\"\n",
    "    RELEVANCE = 1 if:\n",
    "    ans_last_pos  (feature 25)  \n",
    "    ans_length    (feature 28)     \n",
    "    qb_topic_id   (feature 29)\n",
    "\n",
    "    REVEVANCE = 2 if, additionally:\n",
    "    ans_first_pos (feature 24)\n",
    "\n",
    "    RELEVANCE = 3 if, additionally:\n",
    "    ans_first     (feature 21)\n",
    "    ans_last      (feature 22)\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    with open(FILE_READ, 'r') as fread:\n",
    "        for line in fread:\n",
    "            data_list.append(line)\n",
    "\n",
    "    for i, record in enumerate(data_list):\n",
    "        record_list = record.split()\n",
    "        match=0\n",
    "        score1=0\n",
    "        score2=0\n",
    "        score3=0\n",
    "        score4=0\n",
    "        \n",
    "        for k, feature in enumerate(record_list):\n",
    "    \n",
    "            if k==21 or k==22 or k==23 or k==24 or k==25 or k==28 or k==29:\n",
    "                pattern = fr'^{k}:0'\n",
    "                match = re.match(pattern,feature)\n",
    "                \n",
    "                if not match:\n",
    "                    match = 1\n",
    "                else:\n",
    "                    match = 0\n",
    "                \n",
    "                if k==25 or k==28 or k==29:\n",
    "                    score1 += match\n",
    "                elif k==24:\n",
    "                    score2 += match\n",
    "                elif k==21 or k==22:    \n",
    "                    score3 += match\n",
    "                    \n",
    "        if score3==2 and score2==1 and score1==3:\n",
    "            relevance = 3\n",
    "        elif score3!=2 and score2==1 and score1==3:\n",
    "            relevance = 2\n",
    "        elif score3!=2 and score2!=1 and score1==3:\n",
    "            relevance = 1\n",
    "        else:\n",
    "            relevance = 0\n",
    "        \n",
    "        record = record.replace('29:0.0 ','')\n",
    "        record = record.replace('29:1.0 ','')\n",
    "        data_list[i] = str(relevance) + ' ' + record\n",
    "\n",
    "    with open(FILE_WRITE, 'w') as fwrite:\n",
    "        for x in data_list:\n",
    "            fwrite.write(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CREATE DATASETS WITH NO RELEVANCE LABELS** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 1: number of queries: train 152, validation 75, test 77\n",
      "MODEL 2: number of queries: train 152, validation 75, test 77\n"
     ]
    }
   ],
   "source": [
    "train_q, validate_q, test_q = create_query_lists()\n",
    "\n",
    "FEATURE_LIST_ONE = solr.feature_list(1)\n",
    "FEATURE_LIST_TWO = solr.feature_list(2)\n",
    "\n",
    "create_model1_data(FEATURE_LIST_ONE, train_q, validate_q, test_q)\n",
    "create_model2_data(FEATURE_LIST_TWO, train_q, validate_q, test_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PREPEND RELEVANCE LABELS TO DATASETS** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1\n",
    "FILE_READ, FILE_WRITE = get_data('train', 1)  \n",
    "prepend_relevance_label_model_one(FILE_READ, FILE_WRITE)\n",
    "\n",
    "FILE_READ, FILE_WRITE = get_data('validate', 1)  \n",
    "prepend_relevance_label_model_one(FILE_READ, FILE_WRITE)\n",
    "\n",
    "FILE_READ, FILE_WRITE = get_data('test', 1)  \n",
    "prepend_relevance_label_model_one(FILE_READ, FILE_WRITE)\n",
    "\n",
    "# MODEL 2\n",
    "FILE_READ, FILE_WRITE = get_data('train', 2)  \n",
    "prepend_relevance_label_model_two(FILE_READ, FILE_WRITE)\n",
    "\n",
    "FILE_READ, FILE_WRITE = get_data('validate', 2)  \n",
    "prepend_relevance_label_model_two(FILE_READ, FILE_WRITE)\n",
    "\n",
    "FILE_READ, FILE_WRITE = get_data('test', 2)  \n",
    "prepend_relevance_label_model_two(FILE_READ, FILE_WRITE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

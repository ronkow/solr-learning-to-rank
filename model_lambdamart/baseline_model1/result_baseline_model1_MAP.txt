java -jar RankLib-2.14.jar -train ../data/experiment/base_train.txt -test ../data/experiment/base_test.txt -validate ../data/experiment/base_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_baseline.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/base_train.txt
Test data:	../data/experiment/base_test.txt
Validation data:	../data/experiment/base_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_baseline.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/base_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/base_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/base_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4241    | 0.4342    | 
2       | 0.4254    | 0.4342    | 
3       | 0.4258    | 0.4339    | 
4       | 0.4257    | 0.4338    | 
5       | 0.4258    | 0.4339    | 
6       | 0.4258    | 0.4339    | 
7       | 0.4257    | 0.4338    | 
8       | 0.4257    | 0.4338    | 
9       | 0.4267    | 0.4338    | 
10      | 0.4255    | 0.4218    | 
11      | 0.4261    | 0.4161    | 
12      | 0.4277    | 0.4228    | 
13      | 0.427     | 0.4223    | 
14      | 0.4271    | 0.422     | 
15      | 0.4268    | 0.4202    | 
16      | 0.4267    | 0.4207    | 
17      | 0.4264    | 0.4205    | 
18      | 0.4264    | 0.4206    | 
19      | 0.4263    | 0.4167    | 
20      | 0.4258    | 0.4179    | 
21      | 0.4278    | 0.4098    | 
22      | 0.4287    | 0.41      | 
23      | 0.4283    | 0.4098    | 
24      | 0.4285    | 0.4091    | 
25      | 0.4286    | 0.4066    | 
26      | 0.4283    | 0.4065    | 
27      | 0.4283    | 0.4065    | 
28      | 0.4288    | 0.4067    | 
29      | 0.4287    | 0.4068    | 
30      | 0.4281    | 0.4069    | 
31      | 0.4321    | 0.4062    | 
32      | 0.43      | 0.4058    | 
33      | 0.432     | 0.4036    | 
34      | 0.4313    | 0.4019    | 
35      | 0.4327    | 0.3986    | 
36      | 0.4321    | 0.3998    | 
37      | 0.432     | 0.4       | 
38      | 0.4312    | 0.4005    | 
39      | 0.4305    | 0.4008    | 
40      | 0.4297    | 0.3989    | 
41      | 0.4272    | 0.4012    | 
42      | 0.4279    | 0.4014    | 
43      | 0.4254    | 0.4059    | 
44      | 0.4252    | 0.4053    | 
45      | 0.427     | 0.4034    | 
46      | 0.427     | 0.4054    | 
47      | 0.4266    | 0.406     | 
48      | 0.4272    | 0.4066    | 
49      | 0.4275    | 0.407     | 
50      | 0.4262    | 0.4073    | 
51      | 0.4262    | 0.4064    | 
52      | 0.4264    | 0.4072    | 
53      | 0.4279    | 0.4065    | 
54      | 0.426     | 0.4078    | 
55      | 0.427     | 0.4078    | 
56      | 0.4275    | 0.4092    | 
57      | 0.4311    | 0.4085    | 
58      | 0.4295    | 0.4089    | 
59      | 0.4281    | 0.4089    | 
60      | 0.427     | 0.4052    | 
61      | 0.4288    | 0.4051    | 
62      | 0.4302    | 0.4038    | 
63      | 0.4268    | 0.4034    | 
64      | 0.4294    | 0.4026    | 
65      | 0.4292    | 0.4016    | 
66      | 0.4272    | 0.4024    | 
67      | 0.4258    | 0.4022    | 
68      | 0.4261    | 0.4023    | 
69      | 0.4293    | 0.4018    | 
70      | 0.429     | 0.4017    | 
71      | 0.4262    | 0.4036    | 
72      | 0.4264    | 0.4006    | 
73      | 0.4268    | 0.4001    | 
74      | 0.4284    | 0.4047    | 
75      | 0.4299    | 0.4045    | 
76      | 0.4288    | 0.4052    | 
77      | 0.4296    | 0.4049    | 
78      | 0.4287    | 0.405     | 
79      | 0.4287    | 0.4036    | 
80      | 0.4265    | 0.4015    | 
81      | 0.4261    | 0.4023    | 
82      | 0.4264    | 0.4003    | 
83      | 0.4263    | 0.4012    | 
84      | 0.4277    | 0.4006    | 
85      | 0.4283    | 0.4009    | 
86      | 0.429     | 0.4002    | 
87      | 0.4295    | 0.4001    | 
88      | 0.4286    | 0.401     | 
89      | 0.4296    | 0.4007    | 
90      | 0.4284    | 0.4012    | 
91      | 0.4277    | 0.3993    | 
92      | 0.4282    | 0.4015    | 
93      | 0.4289    | 0.3995    | 
94      | 0.4308    | 0.4012    | 
95      | 0.4298    | 0.4006    | 
96      | 0.4304    | 0.3998    | 
97      | 0.4302    | 0.3999    | 
98      | 0.4299    | 0.3986    | 
99      | 0.4289    | 0.3993    | 
100     | 0.4293    | 0.4002    | 
101     | 0.4289    | 0.3979    | 
102     | 0.429     | 0.3973    | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.4241
MAP on validation data: 0.4342
---------------------------------
MAP on test data: 0.391


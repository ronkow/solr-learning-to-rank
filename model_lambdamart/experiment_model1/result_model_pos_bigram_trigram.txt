java -jar RankLib-2.14.jar -train ../data/experiment/pos_bigram_trigram_train.txt -test ../data/experiment/pos_bigram_trigram_test.txt -validate ../data/experiment/pos_bigram_trigram_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_pos_bigram_trigram.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/pos_bigram_trigram_train.txt
Test data:	../data/experiment/pos_bigram_trigram_test.txt
Validation data:	../data/experiment/pos_bigram_trigram_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_pos_bigram_trigram.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/pos_bigram_trigram_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/pos_bigram_trigram_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/pos_bigram_trigram_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4262    | 0.4344    | 
2       | 0.7189    | 0.7289    | 
3       | 0.7438    | 0.7503    | 
4       | 0.7665    | 0.7824    | 
5       | 0.7661    | 0.7816    | 
6       | 0.7976    | 0.8121    | 
7       | 0.798     | 0.8117    | 
8       | 0.795     | 0.8088    | 
9       | 0.7933    | 0.808     | 
10      | 0.7948    | 0.8092    | 
11      | 0.8145    | 0.8191    | 
12      | 0.8127    | 0.8193    | 
13      | 0.8185    | 0.8232    | 
14      | 0.8192    | 0.8222    | 
15      | 0.8204    | 0.822     | 
16      | 0.8264    | 0.8273    | 
17      | 0.8318    | 0.8296    | 
18      | 0.8317    | 0.8291    | 
19      | 0.8354    | 0.8314    | 
20      | 0.8364    | 0.8331    | 
21      | 0.8372    | 0.8364    | 
22      | 0.8377    | 0.836     | 
23      | 0.8383    | 0.8374    | 
24      | 0.8424    | 0.8371    | 
25      | 0.8427    | 0.8376    | 
26      | 0.8427    | 0.8369    | 
27      | 0.8473    | 0.8386    | 
28      | 0.8534    | 0.8445    | 
29      | 0.853     | 0.8451    | 
30      | 0.8511    | 0.8449    | 
31      | 0.8494    | 0.8443    | 
32      | 0.8493    | 0.8454    | 
33      | 0.8484    | 0.8445    | 
34      | 0.85      | 0.8467    | 
35      | 0.8505    | 0.8448    | 
36      | 0.8524    | 0.8444    | 
37      | 0.8517    | 0.8433    | 
38      | 0.8517    | 0.845     | 
39      | 0.8515    | 0.8437    | 
40      | 0.8515    | 0.8436    | 
41      | 0.852     | 0.8455    | 
42      | 0.8513    | 0.8455    | 
43      | 0.8516    | 0.8458    | 
44      | 0.8521    | 0.8465    | 
45      | 0.8511    | 0.8477    | 
46      | 0.8507    | 0.8471    | 
47      | 0.851     | 0.8477    | 
48      | 0.8511    | 0.8479    | 
49      | 0.8494    | 0.8454    | 
50      | 0.8479    | 0.8438    | 
51      | 0.8475    | 0.8431    | 
52      | 0.8475    | 0.8431    | 
53      | 0.8468    | 0.843     | 
54      | 0.8464    | 0.8429    | 
55      | 0.8476    | 0.8445    | 
56      | 0.8477    | 0.8449    | 
57      | 0.8479    | 0.8446    | 
58      | 0.8478    | 0.8441    | 
59      | 0.8481    | 0.844     | 
60      | 0.8496    | 0.8441    | 
61      | 0.8496    | 0.8438    | 
62      | 0.8501    | 0.8438    | 
63      | 0.8505    | 0.843     | 
64      | 0.8499    | 0.8426    | 
65      | 0.8502    | 0.8436    | 
66      | 0.8501    | 0.8431    | 
67      | 0.8502    | 0.8431    | 
68      | 0.8501    | 0.8422    | 
69      | 0.8475    | 0.841     | 
70      | 0.8464    | 0.8404    | 
71      | 0.8459    | 0.8397    | 
72      | 0.8456    | 0.8391    | 
73      | 0.8455    | 0.8394    | 
74      | 0.8452    | 0.8388    | 
75      | 0.845     | 0.838     | 
76      | 0.8456    | 0.8386    | 
77      | 0.845     | 0.8384    | 
78      | 0.8447    | 0.8379    | 
79      | 0.844     | 0.8349    | 
80      | 0.8448    | 0.8354    | 
81      | 0.8444    | 0.8359    | 
82      | 0.8442    | 0.8356    | 
83      | 0.8429    | 0.8352    | 
84      | 0.8429    | 0.8357    | 
85      | 0.8432    | 0.8359    | 
86      | 0.843     | 0.8355    | 
87      | 0.8428    | 0.8356    | 
88      | 0.8428    | 0.8368    | 
89      | 0.8426    | 0.8368    | 
90      | 0.8416    | 0.8356    | 
91      | 0.8414    | 0.8359    | 
92      | 0.8366    | 0.8291    | 
93      | 0.8365    | 0.8292    | 
94      | 0.8359    | 0.8283    | 
95      | 0.836     | 0.8291    | 
96      | 0.8342    | 0.8277    | 
97      | 0.8339    | 0.8272    | 
98      | 0.8342    | 0.8279    | 
99      | 0.8343    | 0.8276    | 
100     | 0.8342    | 0.8276    | 
101     | 0.8339    | 0.8269    | 
102     | 0.8336    | 0.8266    | 
103     | 0.8333    | 0.8265    | 
104     | 0.8333    | 0.8264    | 
105     | 0.8329    | 0.8331    | 
106     | 0.8343    | 0.829     | 
107     | 0.834     | 0.8289    | 
108     | 0.8337    | 0.829     | 
109     | 0.8318    | 0.8266    | 
110     | 0.8313    | 0.8263    | 
111     | 0.8308    | 0.8255    | 
112     | 0.8306    | 0.8252    | 
113     | 0.8305    | 0.8252    | 
114     | 0.831     | 0.8263    | 
115     | 0.8304    | 0.8261    | 
116     | 0.8301    | 0.8258    | 
117     | 0.8303    | 0.8258    | 
118     | 0.8297    | 0.8262    | 
119     | 0.8287    | 0.8237    | 
120     | 0.8286    | 0.8235    | 
121     | 0.8292    | 0.8239    | 
122     | 0.8283    | 0.8236    | 
123     | 0.8292    | 0.8269    | 
124     | 0.8289    | 0.8241    | 
125     | 0.8289    | 0.8245    | 
126     | 0.8284    | 0.8248    | 
127     | 0.8285    | 0.8249    | 
128     | 0.8287    | 0.8284    | 
129     | 0.8284    | 0.8285    | 
130     | 0.8272    | 0.8277    | 
131     | 0.8267    | 0.8276    | 
132     | 0.8256    | 0.827     | 
133     | 0.825     | 0.827     | 
134     | 0.8249    | 0.8257    | 
135     | 0.8248    | 0.8228    | 
136     | 0.8247    | 0.8227    | 
137     | 0.8247    | 0.8234    | 
138     | 0.8248    | 0.8259    | 
139     | 0.824     | 0.8252    | 
140     | 0.824     | 0.8252    | 
141     | 0.8243    | 0.8263    | 
142     | 0.8243    | 0.8262    | 
143     | 0.8247    | 0.8262    | 
144     | 0.8246    | 0.8262    | 
145     | 0.8243    | 0.8259    | 
146     | 0.8243    | 0.826     | 
147     | 0.8237    | 0.826     | 
148     | 0.8239    | 0.8257    | 
149     | 0.8232    | 0.825     | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.8511
MAP on validation data: 0.8479
---------------------------------
MAP on test data: 0.7675

Model saved to: ../model/experiment/model_pos_bigram_trigram.txt

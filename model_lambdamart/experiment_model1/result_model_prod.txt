java -jar RankLib-2.14.jar -train ../data/experiment/prod_train.txt -test ../data/experiment/prod_test.txt -validate ../data/experiment/prod_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_prod.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/prod_train.txt
Test data:	../data/experiment/prod_test.txt
Validation data:	../data/experiment/prod_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_prod.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/prod_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/prod_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/prod_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4296    | 0.4312    | 
2       | 0.4362    | 0.4356    | 
3       | 0.4362    | 0.436     | 
4       | 0.4362    | 0.436     | 
5       | 0.4362    | 0.436     | 
6       | 0.4362    | 0.436     | 
7       | 0.4362    | 0.436     | 
8       | 0.5194    | 0.5292    | 
9       | 0.5327    | 0.5477    | 
10      | 0.5327    | 0.5481    | 
11      | 0.6207    | 0.63      | 
12      | 0.6196    | 0.6266    | 
13      | 0.6192    | 0.6263    | 
14      | 0.6191    | 0.6262    | 
15      | 0.619     | 0.6263    | 
16      | 0.6186    | 0.6258    | 
17      | 0.6205    | 0.6269    | 
18      | 0.6218    | 0.6306    | 
19      | 0.6216    | 0.6307    | 
20      | 0.6284    | 0.6407    | 
21      | 0.6288    | 0.6372    | 
22      | 0.6297    | 0.6391    | 
23      | 0.6314    | 0.6379    | 
24      | 0.6337    | 0.6407    | 
25      | 0.6315    | 0.6364    | 
26      | 0.6318    | 0.6377    | 
27      | 0.6325    | 0.6389    | 
28      | 0.6329    | 0.6391    | 
29      | 0.6313    | 0.6372    | 
30      | 0.634     | 0.6376    | 
31      | 0.6359    | 0.6386    | 
32      | 0.6354    | 0.6417    | 
33      | 0.6343    | 0.6418    | 
34      | 0.6357    | 0.6417    | 
35      | 0.6371    | 0.6387    | 
36      | 0.6382    | 0.6381    | 
37      | 0.6395    | 0.6356    | 
38      | 0.6396    | 0.6363    | 
39      | 0.6383    | 0.6341    | 
40      | 0.6375    | 0.6369    | 
41      | 0.6394    | 0.6381    | 
42      | 0.6394    | 0.6378    | 
43      | 0.6393    | 0.637     | 
44      | 0.6385    | 0.6365    | 
45      | 0.6396    | 0.6361    | 
46      | 0.639     | 0.632     | 
47      | 0.6381    | 0.6327    | 
48      | 0.64      | 0.6358    | 
49      | 0.6397    | 0.6345    | 
50      | 0.6401    | 0.6361    | 
51      | 0.642     | 0.6288    | 
52      | 0.6412    | 0.6284    | 
53      | 0.6397    | 0.6269    | 
54      | 0.6391    | 0.6259    | 
55      | 0.6401    | 0.6267    | 
56      | 0.6412    | 0.6281    | 
57      | 0.6419    | 0.6274    | 
58      | 0.641     | 0.6279    | 
59      | 0.6403    | 0.6266    | 
60      | 0.6407    | 0.6272    | 
61      | 0.641     | 0.6287    | 
62      | 0.6411    | 0.626     | 
63      | 0.645     | 0.6219    | 
64      | 0.6444    | 0.6209    | 
65      | 0.6434    | 0.6203    | 
66      | 0.6418    | 0.6183    | 
67      | 0.6421    | 0.6229    | 
68      | 0.6436    | 0.6216    | 
69      | 0.6417    | 0.622     | 
70      | 0.6393    | 0.6205    | 
71      | 0.6396    | 0.6202    | 
72      | 0.6399    | 0.6206    | 
73      | 0.6403    | 0.6202    | 
74      | 0.6407    | 0.6196    | 
75      | 0.641     | 0.6197    | 
76      | 0.6402    | 0.6192    | 
77      | 0.6399    | 0.6201    | 
78      | 0.6399    | 0.618     | 
79      | 0.6395    | 0.6195    | 
80      | 0.6398    | 0.6183    | 
81      | 0.6403    | 0.618     | 
82      | 0.6407    | 0.6188    | 
83      | 0.6411    | 0.618     | 
84      | 0.6399    | 0.6201    | 
85      | 0.6391    | 0.6181    | 
86      | 0.6375    | 0.6173    | 
87      | 0.6379    | 0.617     | 
88      | 0.6376    | 0.6155    | 
89      | 0.6379    | 0.6155    | 
90      | 0.6369    | 0.6155    | 
91      | 0.6375    | 0.6157    | 
92      | 0.6376    | 0.6168    | 
93      | 0.6366    | 0.6148    | 
94      | 0.6388    | 0.6152    | 
95      | 0.6389    | 0.6157    | 
96      | 0.637     | 0.6154    | 
97      | 0.6368    | 0.6132    | 
98      | 0.6395    | 0.6148    | 
99      | 0.64      | 0.6145    | 
100     | 0.6439    | 0.6127    | 
101     | 0.6442    | 0.6148    | 
102     | 0.6451    | 0.6187    | 
103     | 0.6457    | 0.6182    | 
104     | 0.6478    | 0.6199    | 
105     | 0.6484    | 0.6232    | 
106     | 0.6487    | 0.622     | 
107     | 0.649     | 0.6212    | 
108     | 0.6487    | 0.6213    | 
109     | 0.6491    | 0.623     | 
110     | 0.6489    | 0.6223    | 
111     | 0.6484    | 0.6221    | 
112     | 0.6486    | 0.6232    | 
113     | 0.6479    | 0.6231    | 
114     | 0.6471    | 0.6225    | 
115     | 0.6469    | 0.6235    | 
116     | 0.6467    | 0.6226    | 
117     | 0.6468    | 0.6245    | 
118     | 0.6469    | 0.6238    | 
119     | 0.6459    | 0.6241    | 
120     | 0.6459    | 0.6232    | 
121     | 0.6459    | 0.6235    | 
122     | 0.6456    | 0.6231    | 
123     | 0.6446    | 0.6224    | 
124     | 0.6453    | 0.6247    | 
125     | 0.6448    | 0.6238    | 
126     | 0.6447    | 0.6238    | 
127     | 0.6452    | 0.6241    | 
128     | 0.6444    | 0.6244    | 
129     | 0.6439    | 0.6249    | 
130     | 0.6432    | 0.6235    | 
131     | 0.6429    | 0.6244    | 
132     | 0.6424    | 0.6249    | 
133     | 0.6425    | 0.6245    | 
134     | 0.6423    | 0.6236    | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.6343
MAP on validation data: 0.6418
---------------------------------
MAP on test data: 0.5908

Model saved to: ../model/experiment/model_prod.txt

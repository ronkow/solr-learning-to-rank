java -jar RankLib-2.14.jar -train ../data/experiment/bigram_train.txt -test ../data/experiment/bigram_test.txt -validate ../data/experiment/bigram_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_bigram.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/bigram_train.txt
Test data:	../data/experiment/bigram_test.txt
Validation data:	../data/experiment/bigram_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_bigram.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/bigram_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/bigram_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/bigram_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4264    | 0.4347    | 
2       | 0.4264    | 0.4348    | 
3       | 0.6438    | 0.6692    | 
4       | 0.6417    | 0.6679    | 
5       | 0.6573    | 0.6794    | 
6       | 0.6575    | 0.6791    | 
7       | 0.6575    | 0.6791    | 
8       | 0.6596    | 0.6817    | 
9       | 0.6605    | 0.682     | 
10      | 0.6648    | 0.6856    | 
11      | 0.6665    | 0.6869    | 
12      | 0.6755    | 0.6971    | 
13      | 0.6804    | 0.7091    | 
14      | 0.6802    | 0.7077    | 
15      | 0.6791    | 0.7056    | 
16      | 0.6818    | 0.7091    | 
17      | 0.6815    | 0.7006    | 
18      | 0.681     | 0.6986    | 
19      | 0.7247    | 0.7296    | 
20      | 0.7347    | 0.7362    | 
21      | 0.7362    | 0.7402    | 
22      | 0.7376    | 0.7411    | 
23      | 0.7378    | 0.7416    | 
24      | 0.7556    | 0.7683    | 
25      | 0.7573    | 0.7699    | 
26      | 0.758     | 0.7702    | 
27      | 0.7582    | 0.7699    | 
28      | 0.7569    | 0.7704    | 
29      | 0.7547    | 0.7698    | 
30      | 0.7556    | 0.7719    | 
31      | 0.7546    | 0.7722    | 
32      | 0.7537    | 0.7729    | 
33      | 0.7533    | 0.774     | 
34      | 0.7526    | 0.7739    | 
35      | 0.7536    | 0.7733    | 
36      | 0.7555    | 0.7754    | 
37      | 0.7561    | 0.7744    | 
38      | 0.7582    | 0.775     | 
39      | 0.7585    | 0.7754    | 
40      | 0.7571    | 0.7784    | 
41      | 0.7589    | 0.777     | 
42      | 0.7611    | 0.7775    | 
43      | 0.7623    | 0.7687    | 
44      | 0.7616    | 0.7677    | 
45      | 0.7618    | 0.769     | 
46      | 0.7614    | 0.7698    | 
47      | 0.7609    | 0.7766    | 
48      | 0.7614    | 0.7751    | 
49      | 0.7625    | 0.7676    | 
50      | 0.763     | 0.7641    | 
51      | 0.7643    | 0.7658    | 
52      | 0.765     | 0.7657    | 
53      | 0.7655    | 0.7654    | 
54      | 0.7702    | 0.7661    | 
55      | 0.7696    | 0.7649    | 
56      | 0.7685    | 0.7647    | 
57      | 0.7675    | 0.7614    | 
58      | 0.7672    | 0.7646    | 
59      | 0.7669    | 0.7651    | 
60      | 0.7668    | 0.7637    | 
61      | 0.7676    | 0.7625    | 
62      | 0.7709    | 0.7623    | 
63      | 0.7701    | 0.7542    | 
64      | 0.7695    | 0.7543    | 
65      | 0.7693    | 0.7562    | 
66      | 0.7688    | 0.756     | 
67      | 0.7686    | 0.7561    | 
68      | 0.7684    | 0.7559    | 
69      | 0.7681    | 0.7561    | 
70      | 0.7687    | 0.7572    | 
71      | 0.7685    | 0.7566    | 
72      | 0.7683    | 0.7557    | 
73      | 0.7682    | 0.7555    | 
74      | 0.7691    | 0.7561    | 
75      | 0.7688    | 0.7561    | 
76      | 0.7688    | 0.7567    | 
77      | 0.7678    | 0.7542    | 
78      | 0.7698    | 0.7554    | 
79      | 0.7681    | 0.754     | 
80      | 0.7681    | 0.7553    | 
81      | 0.7662    | 0.7525    | 
82      | 0.7654    | 0.7523    | 
83      | 0.7646    | 0.751     | 
84      | 0.7631    | 0.7496    | 
85      | 0.7634    | 0.75      | 
86      | 0.7633    | 0.749     | 
87      | 0.7632    | 0.7498    | 
88      | 0.7628    | 0.7503    | 
89      | 0.7628    | 0.7506    | 
90      | 0.7625    | 0.7507    | 
91      | 0.762     | 0.7516    | 
92      | 0.7618    | 0.7515    | 
93      | 0.7622    | 0.7511    | 
94      | 0.7621    | 0.7511    | 
95      | 0.7619    | 0.7511    | 
96      | 0.7618    | 0.7513    | 
97      | 0.7606    | 0.7498    | 
98      | 0.7606    | 0.7497    | 
99      | 0.7606    | 0.7515    | 
100     | 0.7608    | 0.75      | 
101     | 0.7606    | 0.7504    | 
102     | 0.7607    | 0.7513    | 
103     | 0.7598    | 0.7519    | 
104     | 0.7597    | 0.7508    | 
105     | 0.7591    | 0.7506    | 
106     | 0.7589    | 0.7496    | 
107     | 0.7587    | 0.7497    | 
108     | 0.7585    | 0.7498    | 
109     | 0.7576    | 0.7489    | 
110     | 0.7573    | 0.7513    | 
111     | 0.7569    | 0.7508    | 
112     | 0.7574    | 0.7513    | 
113     | 0.7572    | 0.7515    | 
114     | 0.7573    | 0.751     | 
115     | 0.7585    | 0.7477    | 
116     | 0.7585    | 0.7517    | 
117     | 0.7588    | 0.7514    | 
118     | 0.7592    | 0.751     | 
119     | 0.7594    | 0.7507    | 
120     | 0.7596    | 0.751     | 
121     | 0.7597    | 0.748     | 
122     | 0.7591    | 0.7468    | 
123     | 0.7588    | 0.7503    | 
124     | 0.7583    | 0.7499    | 
125     | 0.7578    | 0.7495    | 
126     | 0.7581    | 0.7488    | 
127     | 0.7574    | 0.7486    | 
128     | 0.7577    | 0.7493    | 
129     | 0.7569    | 0.7491    | 
130     | 0.7571    | 0.7494    | 
131     | 0.7579    | 0.7491    | 
132     | 0.7574    | 0.7492    | 
133     | 0.7576    | 0.7494    | 
134     | 0.7573    | 0.749     | 
135     | 0.7571    | 0.7488    | 
136     | 0.7568    | 0.7489    | 
137     | 0.7568    | 0.7491    | 
138     | 0.7571    | 0.7489    | 
139     | 0.7571    | 0.7492    | 
140     | 0.757     | 0.7491    | 
141     | 0.7569    | 0.7489    | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.7571
MAP on validation data: 0.7784
---------------------------------
MAP on test data: 0.6863

Model saved to: ../model/experiment/model_bigram.txt

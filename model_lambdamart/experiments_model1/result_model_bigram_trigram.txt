java -jar RankLib-2.14.jar -train ../data/experiment/bigram_trigram_train.txt -test ../data/experiment/bigram_trigram_test.txt -validate ../data/experiment/bigram_trigram_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_bigram_trigram.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/bigram_trigram_train.txt
Test data:	../data/experiment/bigram_trigram_test.txt
Validation data:	../data/experiment/bigram_trigram_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_bigram_trigram.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/bigram_trigram_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/bigram_trigram_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/bigram_trigram_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4262    | 0.4343    | 
2       | 0.4265    | 0.4343    | 
3       | 0.4265    | 0.4343    | 
4       | 0.4341    | 0.4387    | 
5       | 0.4383    | 0.4449    | 
6       | 0.738     | 0.7409    | 
7       | 0.7584    | 0.7711    | 
8       | 0.7616    | 0.7736    | 
9       | 0.7542    | 0.7711    | 
10      | 0.7548    | 0.7723    | 
11      | 0.7552    | 0.7726    | 
12      | 0.7563    | 0.7745    | 
13      | 0.7585    | 0.7752    | 
14      | 0.7691    | 0.78      | 
15      | 0.7739    | 0.7868    | 
16      | 0.7749    | 0.7897    | 
17      | 0.7773    | 0.7881    | 
18      | 0.777     | 0.7891    | 
19      | 0.7788    | 0.7922    | 
20      | 0.7775    | 0.7918    | 
21      | 0.7785    | 0.7936    | 
22      | 0.7784    | 0.7922    | 
23      | 0.7798    | 0.7923    | 
24      | 0.7806    | 0.7928    | 
25      | 0.7803    | 0.7905    | 
26      | 0.7817    | 0.7883    | 
27      | 0.7844    | 0.7914    | 
28      | 0.7865    | 0.7938    | 
29      | 0.7879    | 0.7945    | 
30      | 0.7898    | 0.7971    | 
31      | 0.7877    | 0.7953    | 
32      | 0.7883    | 0.796     | 
33      | 0.7886    | 0.7965    | 
34      | 0.7893    | 0.7965    | 
35      | 0.7909    | 0.7965    | 
36      | 0.7973    | 0.7938    | 
37      | 0.7973    | 0.7931    | 
38      | 0.7981    | 0.7917    | 
39      | 0.7983    | 0.791     | 
40      | 0.7986    | 0.7912    | 
41      | 0.7976    | 0.7903    | 
42      | 0.7976    | 0.7895    | 
43      | 0.7982    | 0.7896    | 
44      | 0.8026    | 0.7886    | 
45      | 0.8035    | 0.7897    | 
46      | 0.8035    | 0.7901    | 
47      | 0.8032    | 0.79      | 
48      | 0.8025    | 0.7902    | 
49      | 0.8034    | 0.7898    | 
50      | 0.808     | 0.7966    | 
51      | 0.8082    | 0.7977    | 
52      | 0.808     | 0.7988    | 
53      | 0.8079    | 0.7976    | 
54      | 0.808     | 0.7963    | 
55      | 0.8081    | 0.7953    | 
56      | 0.8073    | 0.7957    | 
57      | 0.8069    | 0.7953    | 
58      | 0.807     | 0.7955    | 
59      | 0.8061    | 0.7963    | 
60      | 0.805     | 0.7961    | 
61      | 0.8043    | 0.7961    | 
62      | 0.8031    | 0.7969    | 
63      | 0.8031    | 0.7965    | 
64      | 0.8029    | 0.797     | 
65      | 0.8026    | 0.7965    | 
66      | 0.8026    | 0.796     | 
67      | 0.8015    | 0.7962    | 
68      | 0.802     | 0.795     | 
69      | 0.8026    | 0.795     | 
70      | 0.8021    | 0.7955    | 
71      | 0.8013    | 0.7949    | 
72      | 0.801     | 0.7937    | 
73      | 0.8009    | 0.7934    | 
74      | 0.802     | 0.7942    | 
75      | 0.802     | 0.7943    | 
76      | 0.8023    | 0.794     | 
77      | 0.8025    | 0.7943    | 
78      | 0.802     | 0.794     | 
79      | 0.8022    | 0.7938    | 
80      | 0.8017    | 0.793     | 
81      | 0.801     | 0.7927    | 
82      | 0.8011    | 0.7929    | 
83      | 0.8007    | 0.7932    | 
84      | 0.8007    | 0.7929    | 
85      | 0.8011    | 0.793     | 
86      | 0.8009    | 0.7923    | 
87      | 0.8016    | 0.7933    | 
88      | 0.8016    | 0.7927    | 
89      | 0.8017    | 0.7926    | 
90      | 0.8015    | 0.7929    | 
91      | 0.8021    | 0.7935    | 
92      | 0.8022    | 0.7934    | 
93      | 0.8016    | 0.7935    | 
94      | 0.8017    | 0.7941    | 
95      | 0.8016    | 0.7941    | 
96      | 0.8012    | 0.7943    | 
97      | 0.8014    | 0.7947    | 
98      | 0.8021    | 0.7946    | 
99      | 0.8016    | 0.794     | 
100     | 0.8022    | 0.7942    | 
101     | 0.8021    | 0.793     | 
102     | 0.8013    | 0.7929    | 
103     | 0.8012    | 0.7928    | 
104     | 0.8007    | 0.7913    | 
105     | 0.8011    | 0.792     | 
106     | 0.801     | 0.792     | 
107     | 0.8016    | 0.7918    | 
108     | 0.8011    | 0.7909    | 
109     | 0.8011    | 0.7901    | 
110     | 0.8011    | 0.7899    | 
111     | 0.8013    | 0.7893    | 
112     | 0.8011    | 0.7891    | 
113     | 0.8008    | 0.7896    | 
114     | 0.8004    | 0.7892    | 
115     | 0.7994    | 0.788     | 
116     | 0.7995    | 0.788     | 
117     | 0.7992    | 0.7916    | 
118     | 0.799     | 0.7931    | 
119     | 0.7985    | 0.793     | 
120     | 0.7979    | 0.793     | 
121     | 0.7978    | 0.7925    | 
122     | 0.7977    | 0.7919    | 
123     | 0.7978    | 0.7916    | 
124     | 0.7975    | 0.7913    | 
125     | 0.7971    | 0.792     | 
126     | 0.7969    | 0.7915    | 
127     | 0.7971    | 0.7917    | 
128     | 0.797     | 0.7916    | 
129     | 0.7973    | 0.7916    | 
130     | 0.7972    | 0.7916    | 
131     | 0.797     | 0.7913    | 
132     | 0.7976    | 0.7913    | 
133     | 0.7983    | 0.7921    | 
134     | 0.7982    | 0.7921    | 
135     | 0.7982    | 0.7921    | 
136     | 0.7988    | 0.7923    | 
137     | 0.7992    | 0.7926    | 
138     | 0.7995    | 0.7926    | 
139     | 0.8002    | 0.7926    | 
140     | 0.7994    | 0.7925    | 
141     | 0.7995    | 0.7925    | 
142     | 0.7995    | 0.7924    | 
143     | 0.7996    | 0.7924    | 
144     | 0.7998    | 0.7923    | 
145     | 0.8001    | 0.7924    | 
146     | 0.8001    | 0.7917    | 
147     | 0.8003    | 0.7926    | 
148     | 0.8006    | 0.7926    | 
149     | 0.8006    | 0.7922    | 
150     | 0.8005    | 0.7921    | 
151     | 0.8005    | 0.7934    | 
152     | 0.8004    | 0.7941    | 
153     | 0.8003    | 0.794     | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.808
MAP on validation data: 0.7988
---------------------------------
MAP on test data: 0.7329

Model saved to: ../model/experiment/model_bigram_trigram.txt

java -jar RankLib-2.14.jar -train ../data/experiment/pos_train.txt -test ../data/experiment/pos_test.txt -validate ../data/experiment/pos_validate.txt -ranker 6 -metric2t MAP -metric2T MAP -save ../model/experiment/model_pos.txt -gmax 1

Discard orig. features
Training data:	../data/experiment/pos_train.txt
Test data:	../data/experiment/pos_test.txt
Validation data:	../data/experiment/pos_validate.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	MAP
Test metric:	MAP
Feature normalization: No
Model file: ../model/experiment/model_pos.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Min leaf support: 1
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/experiment/pos_train.txt]... [Done.]            
(152 ranked lists, 7600 entries read)
Reading feature file [../data/experiment/pos_validate.txt]... [Done.]            
(75 ranked lists, 3750 entries read)
Reading feature file [../data/experiment/pos_test.txt]... [Done.]            
(77 ranked lists, 3850 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | MAP-T     | MAP-V     | 
---------------------------------
1       | 0.4341    | 0.4364    | 
2       | 0.4456    | 0.4395    | 
3       | 0.5063    | 0.4949    | 
4       | 0.5385    | 0.5283    | 
5       | 0.538     | 0.5216    | 
6       | 0.5424    | 0.5265    | 
7       | 0.5405    | 0.5163    | 
8       | 0.5428    | 0.5139    | 
9       | 0.545     | 0.5152    | 
10      | 0.5472    | 0.5224    | 
11      | 0.5474    | 0.5212    | 
12      | 0.5482    | 0.5203    | 
13      | 0.5467    | 0.5182    | 
14      | 0.5491    | 0.5197    | 
15      | 0.5612    | 0.5327    | 
16      | 0.5636    | 0.5354    | 
17      | 0.5635    | 0.5345    | 
18      | 0.5654    | 0.5316    | 
19      | 0.569     | 0.5336    | 
20      | 0.5682    | 0.5315    | 
21      | 0.5696    | 0.5338    | 
22      | 0.5708    | 0.5498    | 
23      | 0.572     | 0.5476    | 
24      | 0.5747    | 0.5487    | 
25      | 0.5768    | 0.5511    | 
26      | 0.5807    | 0.5523    | 
27      | 0.5828    | 0.5543    | 
28      | 0.5836    | 0.5585    | 
29      | 0.5849    | 0.5567    | 
30      | 0.5855    | 0.5614    | 
31      | 0.5864    | 0.5588    | 
32      | 0.5886    | 0.5614    | 
33      | 0.5926    | 0.5589    | 
34      | 0.5922    | 0.5596    | 
35      | 0.5927    | 0.5594    | 
36      | 0.5933    | 0.5645    | 
37      | 0.5948    | 0.5687    | 
38      | 0.5951    | 0.5677    | 
39      | 0.5971    | 0.5701    | 
40      | 0.596     | 0.5702    | 
41      | 0.596     | 0.5714    | 
42      | 0.5969    | 0.571     | 
43      | 0.5953    | 0.5712    | 
44      | 0.5979    | 0.5696    | 
45      | 0.5969    | 0.5684    | 
46      | 0.5972    | 0.5713    | 
47      | 0.5993    | 0.573     | 
48      | 0.597     | 0.5721    | 
49      | 0.5994    | 0.5749    | 
50      | 0.6019    | 0.573     | 
51      | 0.6006    | 0.5692    | 
52      | 0.603     | 0.57      | 
53      | 0.6019    | 0.5813    | 
54      | 0.6046    | 0.5829    | 
55      | 0.6066    | 0.5841    | 
56      | 0.6054    | 0.5842    | 
57      | 0.6061    | 0.5847    | 
58      | 0.6062    | 0.5852    | 
59      | 0.6051    | 0.5864    | 
60      | 0.604     | 0.5861    | 
61      | 0.6073    | 0.5879    | 
62      | 0.6074    | 0.5878    | 
63      | 0.6086    | 0.5891    | 
64      | 0.6081    | 0.5872    | 
65      | 0.6101    | 0.5887    | 
66      | 0.6107    | 0.5854    | 
67      | 0.608     | 0.5846    | 
68      | 0.609     | 0.585     | 
69      | 0.6063    | 0.5832    | 
70      | 0.6089    | 0.5842    | 
71      | 0.6095    | 0.5843    | 
72      | 0.6105    | 0.5743    | 
73      | 0.608     | 0.575     | 
74      | 0.6073    | 0.5733    | 
75      | 0.6064    | 0.5719    | 
76      | 0.6055    | 0.5713    | 
77      | 0.6068    | 0.5807    | 
78      | 0.6079    | 0.5761    | 
79      | 0.606     | 0.5746    | 
80      | 0.6047    | 0.5841    | 
81      | 0.6024    | 0.5811    | 
82      | 0.6052    | 0.5811    | 
83      | 0.6043    | 0.5822    | 
84      | 0.6037    | 0.5806    | 
85      | 0.6051    | 0.5822    | 
86      | 0.6029    | 0.5807    | 
87      | 0.6022    | 0.5811    | 
88      | 0.6033    | 0.5815    | 
89      | 0.6041    | 0.5821    | 
90      | 0.6024    | 0.5809    | 
91      | 0.6035    | 0.5814    | 
92      | 0.6044    | 0.582     | 
93      | 0.6037    | 0.5826    | 
94      | 0.6039    | 0.5819    | 
95      | 0.6033    | 0.5828    | 
96      | 0.6038    | 0.5823    | 
97      | 0.6045    | 0.5825    | 
98      | 0.6034    | 0.5807    | 
99      | 0.6037    | 0.5806    | 
100     | 0.6014    | 0.5813    | 
101     | 0.6026    | 0.5827    | 
102     | 0.6045    | 0.583     | 
103     | 0.6047    | 0.5842    | 
104     | 0.6042    | 0.5841    | 
105     | 0.604     | 0.5854    | 
106     | 0.6031    | 0.585     | 
107     | 0.6026    | 0.584     | 
108     | 0.6024    | 0.584     | 
109     | 0.6038    | 0.584     | 
110     | 0.6037    | 0.5841    | 
111     | 0.604     | 0.5834    | 
112     | 0.6024    | 0.5844    | 
113     | 0.6021    | 0.5838    | 
114     | 0.6023    | 0.585     | 
115     | 0.6014    | 0.5841    | 
116     | 0.6015    | 0.5845    | 
117     | 0.6025    | 0.5846    | 
118     | 0.6013    | 0.5848    | 
119     | 0.6015    | 0.5851    | 
120     | 0.602     | 0.5853    | 
121     | 0.601     | 0.5853    | 
122     | 0.6013    | 0.5858    | 
123     | 0.6001    | 0.5858    | 
124     | 0.6006    | 0.5863    | 
125     | 0.6008    | 0.5868    | 
126     | 0.5999    | 0.585     | 
127     | 0.5998    | 0.5843    | 
128     | 0.6009    | 0.586     | 
129     | 0.6035    | 0.586     | 
130     | 0.6038    | 0.5864    | 
131     | 0.6044    | 0.5865    | 
132     | 0.6047    | 0.5863    | 
133     | 0.6057    | 0.5863    | 
134     | 0.6059    | 0.5862    | 
135     | 0.6063    | 0.586     | 
136     | 0.6065    | 0.5858    | 
137     | 0.6067    | 0.5858    | 
138     | 0.6073    | 0.5863    | 
139     | 0.6071    | 0.5858    | 
140     | 0.6068    | 0.5852    | 
141     | 0.6067    | 0.5832    | 
142     | 0.6073    | 0.5827    | 
143     | 0.607     | 0.5823    | 
144     | 0.6067    | 0.5828    | 
145     | 0.6062    | 0.5827    | 
146     | 0.6058    | 0.5825    | 
147     | 0.606     | 0.5822    | 
148     | 0.6047    | 0.5818    | 
149     | 0.6042    | 0.5819    | 
150     | 0.6044    | 0.5819    | 
151     | 0.6045    | 0.582     | 
152     | 0.6051    | 0.5814    | 
153     | 0.6057    | 0.5824    | 
154     | 0.605     | 0.5813    | 
155     | 0.6051    | 0.5805    | 
156     | 0.6054    | 0.5817    | 
157     | 0.6051    | 0.5809    | 
158     | 0.6043    | 0.58      | 
159     | 0.6043    | 0.5806    | 
160     | 0.6041    | 0.5791    | 
161     | 0.6054    | 0.58      | 
162     | 0.6051    | 0.5808    | 
163     | 0.6054    | 0.5807    | 
164     | 0.6059    | 0.5813    | 
---------------------------------
Finished sucessfully.
MAP on training data: 0.6086
MAP on validation data: 0.5891
---------------------------------
MAP on test data: 0.5411

Model saved to: ../model/experiment/model_pos.txt
